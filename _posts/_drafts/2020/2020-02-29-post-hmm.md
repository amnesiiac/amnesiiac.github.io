---
layout: post
title: "Hidden Markov Model"
subtitle: 'HMM原理解析'
author: "twistfatezz"
header-style: text
# header-img: "img/post-bg-alitrip.jpg"
# header-mask: 0.1
mathjax: true
date: 2020-02-29 9:53 
lang: ch 
catalog: true 
categories: documentation
tags:
  - graphical model 
  - Time 2020
---

## Introduction
隐马尔科夫框架(Hidden Markov Model, HMM)是一种可用于标注问题的统计学习模型，同时，由隐藏的(难以观测)马尔科夫链随机生成观测序列的过程属于生成模型。隐马尔可夫模型在语音识别、自然语言处理、生物信息、模式识别等领域有着广泛的应用。本文章整理自李航老师的统计学习方法，整理目的在于便于知识回忆。

## Definition 
隐马尔可夫模型是一种关于`时序`的概率模型。首先隐藏的马尔科夫链生成不可观测的`状态随机序列`(state sequence)，然后`状态随机序列`中的各个状态再生成`观测随机序列`(observation sequence)的过程。

<center><img src="/img/in-post/hmm/hmm_overview.pdf" width="80%"></center>

如图所示，$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合。$N$是可能的状态数，$M$是对应的观测序列。

$$
Q={q_1,q_2,\cdots q_N}, \quad V={v_1,v_2,\cdots v_M}
$$

$I$是长度为$T$的状态序列，$O$是对应的观测序列，如图所示有$I\in Q$以及$O\in V$的关系，即$I$和$O$两个随机变量分别服从$Q$和$V$的分布。

$$
I=(i_1,i_2,\cdots i_T), \quad O=(o_1,o_2,\cdots o_T)
$$

如图所示，初始状态$pi_i$由初始状态概率向量$\pi=(\pi_i)$决定。其中，初始状态满足的概率关系为：$\pi_i=P(i_1=q_i),\ i=1,2,\cdots N$。$\pi_i$表示时刻$t=1$处于状态$q_i$的概率。

A表示状态转移矩阵，可定义为$A=\left\[a_{ij}\right\]_{N\times N}$，其中，$$a_{ij}=P(i_{t+1}=q_j\mid i_t=q_i),\ i=1,2 \cdots N; j=1,2, \cdots N$$，表示在时刻$t$处于状态$q_i$的条件下在时刻$t+1$转移到状态$q_j$的概率。

B表示观测概率矩阵，可定义为$B=\left\[b_j(k)\right\]_{N\times M}$，其中，$$b_j(k)=P(o_t=v_k\mid i_t=q_j),k=1,2\cdots M; j=1,2\cdots N$$，表示在时刻$t$处于状态$q_j$的条件下生成观测$v_k$的概率。

<center><img src="/img/in-post/hmm/hmm_overview1.pdf" width="80%"></center>

## Two Assumptions
`1 齐次马尔科夫性假设`:即假设隐藏的马尔科夫链在任意时刻$t$的状体啊只依赖于其前一时刻的状态，于其他时刻的状态及观测无关，也于时刻$t$无关。<br>
`2 观测独立性假设`:即假设任意时刻的观测只依赖于该时刻的马尔科夫状态，于其他观测及状态无关。

<center><img src="/img/in-post/hmm/hmm_overview2.pdf" width="80%"></center>

## An Example
假设手里有三种不同的骰子如下图所示，骰子的质地绝对均匀。$D6$得到的各种点数的概率为$1/6$，同理$D4$得到各个点数概率为$1/4$，$D8$得到各个点数概率为$1/8$。

<center><img src="/img/in-post/hmm/example1.pdf" width="80%"></center>

现在我们随机选择一个骰子，假设每一次选择每个骰子的概率为$1/3$，那么下图中展示的HMM隐藏状态转化关系中的概率将都为$1/3$。

<center><img src="/img/in-post/hmm/example3.pdf" width="80%"></center>

> 其实对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。

## Three problem #1
`1 概率计算问题 已知lambda,O 求P(O|lambda):` 给定模型$\lambda =(A,B,\pi)$和观测序列$O=(o_1,o_2\cdots o_T)$，计算在给定模型$\lambda$参数的前提下，观测序列$O$出现的概率$P(O\mid \lambda)$。 <br>
`2 学习问题 已知O 使用最大似然估计lambda:` 已知观测序列$O=(o_1,o_2\cdots o_T)$，估计模型$\lambda=(A,B,\pi)$的参数，使得在该模型下得到观测序列$O$的概率最大，即使得$P(I\mid O)$最大。应使用最大似然估计方法估计参数。<br>
`3 预测问题(也称为decoding问题) 已知lambda,O 求最有可能的I:` 已知模型参数$\lambda =(A,B,\pi)$和观测序列$O=(o_1,o_2\cdots o_T)$，求对于给定观测序列$O$，求最有可能的对应的状态序列$I$。<br>

## Three problem #2
`1 概率计算问题 已知HMM模型参数、观测序列 求得到这种观测的概率:` 知道骰子有几种(隐含状态数量$\pi$和$A$)，每种骰子是什么(转换概率$B$矩阵)，根据掷骰子掷出的结果(观测矩阵$O$矩阵)，我想知道掷出这个结果的概率。 <br>
> 看似这个问题意义不大，因为你掷出来的结果很多时候都对应了一个比较大的概率。这个问题的意义在于，可以用来检测观察到的特定结果的概率和已知的模型参数lambda是否吻合。如果多次投掷得到的结果中频率非常高的一个观测值对应一个按照模型参数计算本来发生的可能性比较小(即实际和理论发生概率相差较大)，那么就说明我们已知的模型参数lambda很有可能是错的(用于推导理论模型的那些骰子已经和做实验收集观测的骰子不一致了)。<br>

`2 已知观测序列 求最大似然估计HMM参数:` 知道骰子有几种(隐含状态数量$\pi$和$A$)，以及观测到很多次掷骰子的结果(可见观测状态链$O$)，不知道每种骰子是什么(转换概率$B$矩阵)，我想反推观测序列背后隐藏的每种骰子是质地，即均匀程度估计(转换概率$B$矩阵)。<br>
> 这个问题很重要，因为这是最常见的情况。很多时候我们只有可见结果，不知道HMM模型里的参数，我们需要从可见结果估计出这些参数，这是建模的一个必要步骤。<br>

`3 已知HMM模型参数 观测序列 求最可能的状态序列:` 知道骰子有几种(隐含状态数量$\pi$和$A$)，每种骰子是什么(转换概率$B$矩阵)，根据掷骰子掷出的结果(观测序列$O$)，想要求解每次掷出来的都是哪种骰子(隐含状态链$I$)。<br>
> 这个问题在语音识别领域，叫做解码问题，即已知词库所有单词，以及每个单词的出现频率，并且知道任意单词到下一个单词的<关联转换概率>，根据输入的语音，将语音转化成词库中的单词序列。<br>

## Solve the problems
### #1 problem
知道骰子的个数、种类($D6$、$D4$、$D8$)，每次投的骰子是哪一个(隐藏状态$I$序列)，已知投掷骰子的结果(观测$O$序列)，求产生这个观测序列的概率计算方式如下图。

<center><img src="/img/in-post/hmm/solve.pdf" width="40%"></center>

### #2 problem
**➤ 监督型** 训练数据包含$S$个长度相同的观测序列$O_i$和对应的状态序列$I_i$，即训练数据为$$\left \{(O_1,I_1),(O_2,I_2),(O_3,I_3),\cdots,(O_S,I_S)\right \}$$，那么可以利用极大似然估计法来估计隐马尔可夫模型的参数，具体方法如下：

**1** 转移状态矩阵$a_{ij}$的估计: 设样本中时刻$t$处于状态$i$到时刻$t+1$转移到状态$j$的频数为$A_{ij}$，那么状态转移概率$a_{ij}$的估计是:

$$
\hat{a}_{ij}=\frac{A_{ij}}{\sum_{l=1}^{N} A_{ij}}, \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, N
$$

**2** 状态到观测概率矩阵$b_{j}(k)$的估计: 设样本中状态为$j$并观测为$k$的频数是$B_{jk}$，那么状态为$j$观测为$k$的概率$b_{j}(k)$的估计是:

$$
\hat{b}_{j}(k)=\frac{B_{A}}{\sum_{k=1}^{M} B_{j k}}, \quad j=1,2, \cdots, N, \quad k=1,2, \cdots, M
$$

**3** 初始状态概率$\pi_i$的估计$\hat{\pi_i}$为$S$个样本中初始状态为$q_i$的频率。

**➤ 非监督型** 训练数据知包含$S$个长度为$T$的观测序列$$\left\{ O_1,O_2,\cdots,O_S\right\}$$而没有对应的状态序列，目标是学习隐马尔可夫模型$$\lambda=(A,B,\pi)$$的参数。我们将观测序列数据看作观测数据$O$，状态序列数据看作不可观测的隐数据$I$，那么隐马尔可夫模型事实上是一个含有隐变量的概率模型:

$$
P(O \mid \lambda)=\sum_{I} P(O \mid I, \lambda) P(I \mid \lambda)
$$

这是一个含有隐变量的最大似然参数估计问题，这种参数估计可以由[EM算法](/documentation/2020/03/06/post-em/)实现。

### #3 problem
**➤ 近似算法** 其思想可概括为：在每个时刻选择在该时刻最有可能出现的状态$i_t^*$，从而得到一个状态序列$$I^*=(i_1^*, i_2^*, \cdots, i_T^*)$$，将它作为预测的结果。给定HMM模型参数$\lambda$和观测序列$O$，在时刻$t$处于状态$q_i$的概率$\gamma(i)$是:

$$
\gamma_{t}(i)=\frac{\alpha_{t}(i) \beta_{t}(i)}{P(O | \lambda)}=\frac{\alpha_{t}(i) \beta_{t}(i)}{\sum_{j=1}^{N} \alpha_{t}(j) \beta_{t}(j)}
$$

在每一时刻$t$最有可能的状态$i_t^*$是:

$$
i_{t}^{*}=\arg \max _{1\leq i \leq N} [\gamma_{t}(i)], \quad t=1,2, \cdots, T
$$

从而得到状态序列$$I^*=(i_1^*, i_2^*, \cdots, i_T^*)$$。

**➤ 维特比算法** `viterbi algorithm`是使用动态规划求解隐马尔可夫模型预测问题，即用动态规划求概率最大路径，这条路径对应了最优的状态序列。比较容易理解，直接参考`统计学习方法`p186页中的例10.3即可。其基本思想是从$t=1$时刻，按照时刻顺序，逐一推演当前$t$时刻能够满足观测输出的`最可能`的状态$I_t$，最终构成求解的状态序列$$I^*=(i_1, i_2, \cdots, i_T)$$。

## Reference
> 李航 - 统计学习方法 <br>
> https://www.kancloud.cn/thinkphp/hmm/45496 <br>

> 1 当使用inline数学公式且公式经过GFM排版之后都在同一行 使用`$...$`符号<br>
> 2 当希望数学公式单独成行或者经过GFM排版之后占用多行 应当使用`$$...$$`符号<br>
> 3 对于表示条件概率 需要表示竖线的时候`|` 应当使用`\mid` 而不是直接在键盘上打出`|` => 容易被编辑器认为是一个md制表符<br>
> 4 在md引入图片的时候 不要使用`<center>`和`</center>` 在这篇文档的编辑过程中vscode的preview插件在使用了上述符号之后 导致下一段的数学公式预览显示不正常<br>
> 5 使用md的时候 单独的两段文字上下需要空出一行<br>
> 6 想要强制换行的时候 需要使用`<br>`而不是`<enter>`<br>
> 7 特殊字符如果想要避免和md解析关键字冲突 应当使用``将关键字包含在内