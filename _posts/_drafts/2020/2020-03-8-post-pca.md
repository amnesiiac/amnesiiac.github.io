---
layout: post
title: "Principal component analysis"
subtitle: 'PCA原理解析'
author: "twistfatezz"
header-style: text
# header-img: "img/post-bg-alitrip.jpg"
# header-mask: 0.1
mathjax: true
date: 2020-03-08 17:32 
lang: ch 
catalog: true 
categories: documentation
tags:
  - statistics theroy
  - Time 2020
---

## Introduction
principal component analysis(pca)是一种数据降维的方法，顾名思义，用来获得一组数据的主要特征。本文主要参考CS229 lecture notes中对于pca主成分分析的原理解析，结合自己的一些理解进行知识搬运整理，便于后续进行知识回顾。

## Problem
`problem #1:` 给定具有m个数据的数据集${x^{(i)}; i=1,2,\cdots,m}$，数据中包含了多种不同类型的汽车，每个数据样本具有$n$维量化特征($x^{(i)}\in \mathbb{R}^n$)，如汽车极速，转弯半径...，其中对于极速特征的描述采用了两种计量单位：英里/每小时，公里/每小时。显然，这两个特征是线性相关的特征，因此，去掉一个不影响特征空间对于数据样本的描述，因此有$x^{(i)}\in \mathbb{R}^{(n-1)}$，那么我们如何对于特征进行筛选，保留最有效的特征组合来描述我们的数据样本呢？

`problem #2:` 给定一个无线遥控模型飞机调查结果构建的数据集${x^{(i)}; i=1,2,\cdots,m}$，数据集中的数据由两个特征进行描述，遥控者的遥控技术熟练程度$x_1^{(i)}$，以及遥控者在遥控模型飞机时获得的乐趣多少$x_2^{(i)}$。

> Because Radio Controlled helicopters are very difficult to fly, only the most committed students, ones that truly enjoy flying, become good pilots. So, the two attributes x1 and x2 are strongly correlated.

在上述客观条件限制下，构建一个aritficial dataset如下图所示：

<center><img src="/img/in-post/pca/problem2.pdf" width="60%"></center>

`problem #3` 给定一组经过标准化的数据样本如下图中最左边图所示，分别选定两个正交方向$u_1$和$u_2$进行投影，则观察可知，投影所得样本数值方差较大的方向更加能够表征数据样本特征。

<center><img src="/img/in-post/pca/contrast.pdf" width="80%"></center>

## Method
**step-1 标准化** <br>
`1` 求数据集中的均值：$\mu=\frac{1}{m}\sum_{i=1}^{m} x^{(i)}$. <br>
`2` 去均值：$x^{(i)}=x^{(i)}-\mu$.  去均值可以将数据数值减少，且不影响数据形态。<br>
`3` 求方差：$\sigma_j^2=\frac{1}{m}\sum_i(x_j^{(i)})^2$. <br>
`4` 去方差：$x_j^{(i)}=x_j^{(i)}/\sigma_j$.  去方差可以去掉各维特征量纲，避免单位不同影响特征权重。如problem #1中极速为$200km/h$，座位数量为$4$个，在特征分析的时候，易引起依赖极速而忽略座位数量的情况。

**step-2 数据特征协方差矩阵(以3维特征为例)** <br>

$$
C=\left(\begin{array}{ccc}\operatorname{cov}(x, x) & \operatorname{cov}(x, y) & \operatorname{cov}(x, z) \\ \operatorname{cov}(y, x) & \operatorname{cov}(y, y) & \operatorname{cov}(y, z) \\ \operatorname{cov}(z, x) & \operatorname{cov}(z, y) & \operatorname{cov}(z, z)\end{array}\right)
$$

**step-3 求协方差矩阵的特征值和特征向量** <br>
**step-4 将特征值从大到小排序，选择top-k** <br>
假设最大的$k$个特征向量为$(u_1,u_2,\cdots,u_k)$，将所有的特征向量标准化后，组成特征向量矩阵$w$
**step-5 将样本点投影到选取的特征向量上** <br>
对样本集中的每一个样本$x^{(i)}$，转化为新的样本$z(i)=u^Tx^{(i)}$，得到输出样本$(z^{(1)},z^{(2)},\cdots,z^{(m)})$

<center><img src="/img/in-post/pca/projection.pdf" width="60%"></center>

## Go further 
这部分内容主要对于pca原理进行简单推导。<br>
### 最大方差法(matrix)
`1 样本方差 <-> 特征协方差:` 所有样本为经过标准化的数据，因此，投影后均值为0，故投影后的样本点方差可以写成如下的形式，其中右边括号内部为样本特征协方差矩阵：

$$
\frac{1}{m} \sum_{i=1}^{m}\left(x^{(i)^{T}} u\right)^{2} =\frac{1}{m} \sum_{i=1}^{m} u^{T} x^{(i)} x^{(i)^{T}} u = u^{T}\left(\frac{1}{m} \sum_{i=1}^{m} x^{(i)} x^{(i)^{T}}\right) u 
$$

`2 公式形式简化 - 采用基本符号代替:` 

$$
令 \lambda = \frac{1}{m} \sum_{i=1}^{m}\left(x^{(i)^{T}} u\right)^{2}, \quad 令 \sum=\frac{1}{m} \sum_{i=1}^{m} x^{(i)} x^{(i)^{T}}, \quad 上式可写成:\quad \lambda=u^T\sum u
$$

`3 利用u的性质进行推导简化:` 

$$
u是单位向量, u^Tu=1 \quad \Rightarrow \quad \lambda = \lambda * 1 = \lambda u^Tu=u^T\sum u
$$

`4 利用lambda的性质进行推导简化` 

$$
\lambda 是一个标量，\lambda u^Tu=u^T\lambda u \quad \Rightarrow \quad u^T\lambda u=u^T\sum u \quad \Rightarrow \quad \lambda u=\sum u
$$

`5 根据推导结果得出结论`

$$
\sum u=\lambda u \quad 根据特征向量和特征值的定义易知: \quad \lambda是\sum 矩阵的特征值 \quad u是对应的特征向量 \\
最大化投影方差 \Leftrightarrow 最大化\lambda \Leftrightarrow 即协方差矩阵\sum 的最大特征值对应的特征向量u的方向即为最佳投影方向。
$$

### 最大方差法(lagrange)
`1 目标函数`

$$
argmax(投影方差)=argmax(\lambda)=\max_{\mathbf{u}}(u^T\sum u), \ s.t. \ u^Tu=1
$$

`2 构建Lagrange函数 证明lambda是协方差矩阵特征值`

$$
\max _{u} F(u, \lambda)=\max _{u}\left(u^{T} \sum u +\lambda\left(1-u^{T} u \right)\right) \\ 
\frac{\partial F}{\partial u_{i}}=2 \sum u_{i}-2 \lambda u_{i}=0 \  \ \Rightarrow \ 
\sum u=\lambda u
$$

`3 优化结果简化`

$$
max(u^Tu)=max(u^T\lambda u)=max(\lambda u^Tu)=max(\lambda)=max(协方差矩阵\sum 特征值)
$$

### 最小平方误差法
关于这种方式的推导过程可以参考：
> https://www.cnblogs.com/jerrylead/archive/2011/04/18/2020216.html

## Pros & Cons 
`pros:` 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。各主成分之间正交，可消除原始数据成分间的相互影响的因素。计算方法简单，主要运算是特征值分解，易于实现。<br>
`Cons:` 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。PCA一般不用来做直接的特征提取而是用来做特征矩阵的降维。PCA算法无法清楚确定需要保留的主要特征数量。

## Application
`1` 当提取得到的特征数量过多，甚至多超过数据集样本数时，降维是必须的。根据**curse of dimensionality**，数据特征维度越高，数据在每个特征维度分布就越稀疏，对机器学习算法基本是灾难性影响。<br>
`2` 当提取得到的特征有比较明显的自相关的时候，可以考虑降维。<br>
`3` 有时并不是需要对数据进行降维，希望通过pca去除数据噪声。如**problem #2**中skill和enjoyment之间只有$u_1$方向需要保留，而$u_2$方向不能描述数据特征，属于无关噪声，可通过pca进行去除。<br>
`4` 有的时候需要对于高纬数据可视化操作，可以使用pca将数据降维至2维或3维。
`5` 关于数据降维的有益作用：数据降维可以节省算力，并且还可以降低潜在的数据模型的复杂度，并有助于避免模型陷入过拟合(衡量模型相对于一个特定问题维度空间的复杂度的指标是[vc维](/documentation/2020/03/15/post-vc/)，能够完成任务的模型中选择vc维最小的那个，奥卡姆剃刀)。

## Reference
> CS229 Lecture notes 10
> A tutorial on Principal Components Analysis
> python实现pca可以使用scikit-learn docs中的代码
> https://zhuanlan.zhihu.com/p/32412043
> https://www.zhihu.com/question/35666712

> 1 当使用inline数学公式且公式经过GFM排版之后都在同一行 使用`$...$`符号<br>
> 2 当希望数学公式单独成行或者经过GFM排版之后占用多行 应当使用`$$...$$`符号<br>
> 3 对于表示条件概率 需要表示竖线的时候`|` 应当使用`\mid` 而不是直接在键盘上打出`|` => 容易被编辑器认为是一个md制表符<br>
> 4 在md引入图片的时候 不要使用`<center>`和`</center>` 在这篇文档的编辑过程中vscode的preview插件在使用了上述符号之后 导致下一段的数学公式预览显示不正常<br>
> 5 使用md的时候 单独的两段文字上下需要空出一行<br>
> 6 想要强制换行的时候 需要使用`<br>`而不是`<enter>`<br>
> 7 特殊字符如果想要避免和md解析关键字冲突 应当使用``将关键字包含在内