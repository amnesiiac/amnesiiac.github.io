---
layout: post
title: "Gradient boosting decision tree"
subtitle: 'gbdt算法原理解析'
author: "twistfatezz"
header-style: text
# header-img: "img/post-bg-alitrip.jpg"
# header-mask: 0.1
mathjax: true
date: 2020-03-16 17:52
lang: ch 
catalog: true 
categories: documentation
tags:
  - ensemble learning
  - Time 2020
---

## Introduction
gbdt是梯度提升决策树(gradient boosting decision tree)的简称，是最好的机器学习算法之一(最好的打比赛算法之一)。它可以用于分类和回归问题，分类和回归时的基学习器都是[cart回归树]()，两种任务都是在拟合残差。gbdt和[adaboost]()都可以用前向分布算法进行描述，不同之处在于adaboost每次迭代时输入样本的权重不同，重点关注上一次迭代中误分类的样本，而gbdt每次迭代都是在弥补上一次迭代遗留的残差。本文主要基于李航老师的统计学习方法进行知识归纳整理，方便进行知识回顾。

## Preparations
### Forward stagewise
**➤ 前向分布算法简析** <br>
考虑加法模型(additive model)，其中$b(x;\gamma_m)$为基函数(基分类器)，$\gamma_m$为基函数的参数，$\beta_m$为基函数的系数(权重)：

$$
f(x)=\sum_{m=1}^M \beta_mb(x;\gamma_m)
$$

在给定训练数据以及损失函数$L(y,f(x))$的情况下，学习加法模型$f(x)$成为经验风险极小化问题：

$$
\min _{\beta_{m}, \gamma_{m}} \sum_{i=1}^{N} L\left(y_{i}, \sum_{m=1}^{M} \beta_{m} b\left(x_{i} ; \gamma_{m}\right)\right)
$$

前向分布算法(forward stagewise algorithm)求解上述优化问题的方法是：每一步只学习一个基函数模型系数，逐步逼近整个加法模型的优化目标，那么就可以简化优化复杂度。即，每步只需要优化如下的损失函数：

$$
\min_{\beta,\gamma} \sum_{i=1}^N L(y_i,\beta b(x_i;\gamma))
$$

**➤ 前向分布算法步骤** <br>
输入：训练数据集$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$，损失函数$L(y,f(x))$，分类基函数集合${b(x;\gamma)}$。输出：加法模型。

`1` 初始化加法模型$f_0(x)=0$ <br>
`2` 对$m=1,2,\dots,M$: <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`a` 极小化损失函数得到参数$\beta_m$和$\gamma_m$：

$$
\left(\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, \gamma} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta b\left(x_{i} ; \gamma\right)\right)
$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`b` 更新加法模型：$$f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)$$ <br>
`3` 得到迭代完成的加法模型：$$f(x)=f_M(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)$$

### Boosting tree
**➤ 提升树算法简析** <br>
以决策树为分类基函数的提升方法成为提升树。提升树分为二叉回归树和二叉分类树。提升树可以表示为决策树的加法模型，其中$T(x;\Theta_m)$表示决策树，$\Theta$：决策树的参数，$M$：树的个数。

$$
f_M(x)=\sum_{m=1}^M T(x;\Theta_m)
$$

初始提升树$f_0(x)=0$，迭代到第$m$步的模型为，其中$f_{m-1}(x)$为上一步迭代的模型：

$$
f_m(x)=f_{m-1}(x)+T(x;\Theta_m)
$$

通过经验风险极小化确定下一棵决策树的参数$\Theta_m$：

$$
\hat{\Theta}_{m}=\arg \min _{\Theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \Theta_{m}\right)\right)
$$

**➤ 提升树算法步骤** <br>
输入：训练数据集$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$。输出：提升树$f_M(x)$。

`1` 初始化$f_0(x)=0$ <br>
`2` 对于$m=1,2,\dots,M$: <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`a` 计算残差：$$r_{mi}=y_i-f_{m-1}(x_i),\ i=1,2,\dots,N$$ <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`b` 拟合残差$r_{mi}$学习一个回归树，得到$T(x;\Theta_m)$ <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`c` 更新$$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$$ <br>
`3` 得到回归问题提升树$$f_M(x)=\sum_{m=1}^M T(x;\Theta)$$

## GBDT-algorithm 
提升树利用加法模型和前向分步算法实现学习优化过程，当损失函数是平方误差函数(mse)或者指数损失函数时，每一步优化非常简单。然而对于一般损失函数而言，每步优化没有那么容易。Freidman提出了梯度提升的方法，这是一种最速下降法的近似方法，关键是利用损失函数负梯度在当前模型的值作为提升树回归问题中需要拟合的残差近似值：

$$
-\left[\frac{\partial L\left(y, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{m-1}(x)}
$$

**➤ GBDT算法步骤** <br>
输入：训练数据集$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$，损失函数$L(y,f(x))$。输出：提升树$\hat{f}(x)$

`1` 初始化$$f_0(x)=\arg \min _{c}\sum_{i=1}^N L(y_i,c)$$ <br>
`2` 对于每一个回归树$m=1,2,\dots,M$，计算: <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`a` 对于数据集中的每一个样本$i=1,2,\dots,N$，计算:

$$
r_{mi}=-\left[\frac{\partial L\left(y, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{m-1}(x)}
$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`b` 对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj}$，$j=1,2,\dots,J$ <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`c` 对于每个叶结点$j=1,2,\dots,J$，计算其输出$$c_{mj}=\arg \min _{c}\sum_{x_i\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$$ <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`d` 更新$f_m(x)=f_{m-1}(x)+\sum_{j=1}^J c_{mj} I(x\in R_{mj})$ <br>
`3` 得到回归树$\hat{f}(x)=f_M(x)=\sum_{m=1}^M\sum_{j=1}^Jc_{mj}I(x\in R_{mj})$

gbdt算法第一步初始化，估计使损失函数极小化的常数值，是一个只有一个根结点的树。第二步计算损失函数的负梯度在当前模型下的值，作为对于残差的估计。对于平方误差损失函数，估计值等于真正的残差，对于一般损失函数，为残差的一个近似。

### GBDT loss-function
对于分类问题可以选用指数损失函数、对数损失函数。对于回归问题可以选用均方差损失函数、绝对损失函数。另外还有huber损失函数和分位数损失函数，也是用于回归问题，可以增加回归问题的健壮性，可以减少异常点对损失函数的影响，参见[常用损失函数总结]()。

### GBDT regularization
在adaboost中我们会对每个模型乘上一个弱化系数(正则化系数)，减小每个模型对提升的贡献(注意：这个系数和模型的权重不一样，是在权重上又乘以一个0-1之间的小数)，在gbdt中我们采用同样的策略，对于每个模型乘以一个系数$\lambda (0 < \lambda \leq 1)$，降低每个模型对拟合损失的贡献，这种方法也意味着我们需要更多的基学习器。

### GBDT pros & cons
**➤ Pros** <br>
`1` 可以灵活的处理各种类型的数据 <br>
`2` 预测的准确率高 <br>
`3` 使用了一些健壮的损失函数，如huber，可以很好的处理异常值 <br>

**➤ cons** <br>
由于基学习器之间的依赖关系(每个基分类器之间学习有严格的顺序)，难以并行化处理，不过可以通过子采样的SGBT来实现部分并行。
<!-- <center><img src="/img/in-post/vc/vc0.pdf" width="80%"></center> -->

## XGBoost和GBDT的区别
`1` 将树模型的复杂度加入到正则项中，来避免过拟合，因此泛化性能会优于GBDT。<br>
`2` 损失函数是用泰勒展开式展开的，同时用到了一阶导和二阶导，可以加快优化速度。 <br>
`3` 和GBDT只支持CART作为基分类器之外，还支持线性分类器，在使用线性分类器的时候可以使用L1，L2正则化。 <br>
`4` 引进了特征子采样，像RandomForest那样，这种方法既能降低过拟合，还能减少计算。 <br>
`5` 在寻找最佳分割点时，考虑到传统的贪心算法效率较低，实现了一种近似贪心算法，用来加速和减小内存消耗，除此之外还考虑了稀疏数据集和缺失值的处理，对于特征的值有缺失的样本，XGBoost依然能自动找到其要分裂的方向。 <br>
`6` XGBoost支持并行处理，XGBoost的并行不是在模型上的并行，而是在特征上的并行，将特征列排序后以block的形式存储在内存中，在后面的迭代中重复使用这个结构。这个block也使得并行化成为了可能，其次在进行节点分裂时，计算每个特征的增益，最终选择增益最大的那个特征去做分割，那么各个特征的增益计算就可以开多线程进行。 <br>

## Reference
> 李航 - 统计学习方法
> https://www.cnblogs.com/jiangxinyang/p/9248154.html

> 1 当使用inline数学公式且公式经过GFM排版之后都在同一行 使用`$...$`符号<br>
> 2 当希望数学公式单独成行或者经过GFM排版之后占用多行 应当使用`$$...$$`符号<br>
> 3 对于表示条件概率 需要表示竖线的时候`|` 应当使用`\mid` 而不是直接在键盘上打出`|` => 容易被编辑器认为是一个md制表符<br>
> 4 在md引入图片的时候 不要使用`<center>`和`</center>` 在这篇文档的编辑过程中vscode的preview插件在使用了上述符号之后 导致下一段的数学公式预览显示不正常<br>
> 5 使用md的时候 单独的两段文字上下需要空出一行<br>
> 6 想要强制换行的时候 需要使用`<br>`而不是`<enter>`<br>
> 7 特殊字符如果想要避免和md解析关键字冲突 应当使用``将关键字包含在内