---
layout: post
title: "Expectation Maximization algorithm"
subtitle: 'EM算法原理解析'
author: "twistfatezz"
header-style: text
# header-img: "img/post-bg-alitrip.jpg"
# header-mask: 0.1
mathjax: true
date: 2020-03-06 9:53 
lang: ch 
catalog: true 
categories: documentation
tags:
  - parameter estimation
  - Time 2020
---

## Introduction
EM算法是一种非常的算法，在[隐马尔科夫模型HMM](/documentation/2020/02/29/post-hmm/)的模型$\lambda$参数估计问题以及[高斯混合模型GMM](/documentation/2020/03/06/post-gmm/)模型中确定参数的问题中都有很重要的作用。另外，EM算法还可以用于贝叶斯网络中。EM算法非常适合处理含有隐变量的最大似然方法估计参数的问题。下面主要介绍EM的主要推导过程。

## Preparations
**➤ 凸函数** <br>
设$f$是定义域为实数的函数，如果对于所有的实数$x$，$f^{\prime \prime}(x)\geq 0$，那么$f$是凸函数。当$x$是向量时，如果其$Hessian$矩阵$H$是半正定的($H\geq 0$)，那么$f$是凸函数。如果$f^{\prime \prime}(x)>0$或者$H>0$，那么称$f$是严格凸函数。

**➤ Jensen不等式** <br>
如果$f$是凸函数，$X$是随机变量，那么$$E[f(X)]\geq f(E[X])=f(EX)$$，特别地，如果$f$是严格凸函数，那么$$E[f(X)] = f(E[X])=f(EX)$$当且仅当$p(x=E[X])$=1，也就是说$X$是常量。如下图，直观进行解释。需要注意的是，凹凸的称呼不同的教材可能有所不同，采用统一称呼即可。

<center><img src="/img/in-post/em/jensen.pdf" width="60%"></center>

**➤ Lazy statistician** <br>
统计学中计算期望的时候，可以通过`ls`规则简化计算。
> 设$Y$是随机变量$X$的函数，$Y=g(X)$是$Y$关于$X$的连续函数，则有: <br>
> `1` X为离散型随机变量，它的分布律为$P(X=x_k)=p_k,\ k=1,2,\dots$。若$\sum_{k=1}^\infty$绝对收敛，则有$E(Y)=E\[g(X)\]=\sum_{k=1}^\infty g(x_k)p_k$。 <br>
> `2` X是连续型随机变量，它的概率密度为$f(x)$。若$\int_{-\infty}^{+\infty}g(x)f(x)dx$绝对收敛，则有$E(Y)=E\[g(X)\]=\int_{-\infty}^{+\infty}g(x)f(x)dx$

## Problem
给定的训练样本是$$\{x^{1},x^{2}\cdots x^{m}\}$$，样例间独立，我们想找到每个样例隐含的类别$z$，能使得$p(x,z)$最大。$p(x,z)$的最大似然估计如下：

$$
\begin{aligned}
\ell(\theta) &=\sum_{i=1}^{m} \log p(x ; \theta) = \sum_{i=1}^{m} \log \sum_{z} p(x, z ; \theta)
\end{aligned}
$$

`1:`对极大似然函数取对数，`2:` 对每个样例$\{x^{(i)}\}$的每个可能类别$z$求联合分布概率和。因为有隐藏变量$z$存在，但是直接使用这种最大似然的方式确定参数$\theta$比较困难。如果能够先确定了隐含变量$z$后，求解容易很多。

## Method
EM算法就采用了一种`放缩`的方式解决这个问题。既然由于含有隐变量$z$，不能够直接最大化似然函数$l(\theta)$，我们可以采用`逐步放缩`的方式不断地建立似然函数$l$的下界(`E步`)，然后优化下界(`M步`)。

### Preparations
对于每个样例$x^{i}$，令$Q_i$表示该样例的隐含随机变量$z_i$的分布，假定随机变量$z$都是离散的(连续的同理)，那么$Q_i$满足作为分布的基本要求:

$$
\sum_z Q_i(z)=1,\ Q_i(z)\geq 0
$$

根据最大似然函数的表达式，并转化为带有隐含变量的形式，得到：

$$
\sum_{i} \log p\left(x^{(i)} ; \theta\right) =\sum_{i} \log \sum_{z(i)} p\left(x^{(i)}, z^{(i)} ; \theta\right)
$$

将上面关于$Q$的公式带入到最大似然函数的表达式中，分子分母同时乘相同的因子，得到：

$$
=\sum_{i} \log \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)} 
$$

利用Jensen不等式，$f()=log()$为凸函数，因此有$f(EX) \leq E\[f(x)\]$，得到`lowb`：

$$
\geq \sum_{i} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)} = \sum_{i} E[ \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} ] = lowboundary(lowb)
$$

根据`ls`准则，有如下的对应关系:

$$
Y=[ p(x^{(i)},z^{(i)};\theta)/Q_i(z^{(i)}) ];\quad X=z^{(i)};\quad Q_i(z^{(i)})=p_k \\
g() = 从 \ z^{(i)}\ 到\ [ p(x^{(i)},z^{(i)};\theta)/Q_i(z^{(i)}) ]\ 的映射
$$

### E-step

上述准备工作得到的结果相当于为似然函数$l(\theta)​$构建了一个下界`lowb`。对于那么$l(\theta)​$下界主要由$\theta​$和$Q_i(z^{(i)})​$以及$p(x^{(i)},z^{(i)})​$三部分共同决定。如果我们在`E`中将$\theta​$参数固定，则当前`lowb_theta`下界由后面两部分决定。我们可以不断地通过调整$Q_i(z^{(i)})​$和$p(x^{(i)},z^{(i)})​$使得下界`lowb_theta`逐渐提升，根据Jensen不等式取得等号的条件，当随机变量为常量时取等号，即：

$$
\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} = c \quad p(x^{(i)},z^{(i)}; \theta) = c \cdot Q_i(z^{(i)})
$$

已知$\sum_z Q_i(z^{(i)})=1​$，则对于上面式子左右同时积分，得到：

$$
\sum_z p(x^{(i)},z^{(i)};\theta) = c
$$

于是将上面的等式中带入左右积分之前的等式中将$c$进行替换，得到(E步中$\theta$是定值)：

$$
\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} = \sum_z p(x^{(i)},z^{(i)};\theta) \\
Q_i(z^{(i)}) = \frac{p(x^{(i)},z^{(i)};\theta)}{\sum_z p(x^{(i)},z^{(i)};\theta)} = \frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)} = p(z^{(i)} \mid x^{(i)};\theta)
$$

至此，`E-step`可以总结为：固定$\theta$，利用求解$Q_i(z^{(i)})= p(z^{(i)} \mid x^{(i)};\theta)$的过程。

### M-step 
有了上面的推导作为基础，则可以总结为：将`E-step`中得到的$Q_i(z^{(i)})$(固定)，计算$\theta$使得`lowb`函数最大的过程。

$$
\theta=\arg \max _{\theta} \sum_{i} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)}
$$

## EM convergence
这一部分主要介绍EM算法的收敛性。

<center><img src="/img/in-post/em/convergence.pdf" width="80%"></center>

> When the algorithm reaches a fixed point for some $\theta_n$ the value $\theta_n$ maximizes $l(\theta \mid \theta_n)$. Since L and l are equal at $\theta_n$ if $L$ and `lowb` are differentiable at $\theta_n$, then $\theta_n$ must be a stationary point of $L$. The stationary point need not, however, be a local maximum. It is possible for the algorithm to converge to `local minima` or `saddle points` in unusual cases.

<center><img src="/img/in-post/em/convergence1.pdf" width="60%"></center>

根据上图所示，易知有如下的关系，即证明了`EM`算法在运行的过程中是单调增加的。

$$
\begin{aligned}
\ell\left(\theta^{(t+1)}\right) & \geq \sum_{i} \sum_{z^{(i)}} Q_{i}^{(t)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta^{(t+1)}\right)}{Q_{i}^{(t)}\left(z^{(i)}\right)} \\
& \geq \sum_{i} \sum_{z^{(i)}} Q_{i}^{(t)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta^{(t)}\right)}{Q_{i}^{(t)}\left(z^{(i)}\right)} =\ell\left(\theta^{(t)}\right)
\end{aligned}
$$

那么当$\ell(\theta)$数值不再增加或者每轮次迭代增量小于预先设定的数值如$\delta=10^{-8}$，可认为已经收敛。

## Reference
> 李航 - 统计学习方法 HMM中的BW算法(EM)<br>
> https://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html
> http://www.seanborman.com/publications/EM_algorithm.pdf

> 1 当使用inline数学公式且公式经过GFM排版之后都在同一行 使用`$...$`符号<br>
> 2 当希望数学公式单独成行或者经过GFM排版之后占用多行 应当使用`$$...$$`符号<br>
> 3 对于表示条件概率 需要表示竖线的时候`|` 应当使用`\mid` 而不是直接在键盘上打出`|` => 容易被编辑器认为是一个md制表符<br>
> 4 在md引入图片的时候 不要使用`<center>`和`</center>` 在这篇文档的编辑过程中vscode的preview插件在使用了上述符号之后 导致下一段的数学公式预览显示不正常<br>
> 5 使用md的时候 单独的两段文字上下需要空出一行<br>
> 6 想要强制换行的时候 需要使用`<br>`而不是`<enter>`<br>
> 7 特殊字符如果想要避免和md解析关键字冲突 应当使用``将关键字包含在内