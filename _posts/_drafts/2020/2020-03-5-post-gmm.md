---
layout: post
title: "Gaussian Mixed Model"
subtitle: 'GMM原理解析'
author: "twistfatezz"
header-style: text
# header-img: "img/post-bg-alitrip.jpg"
# header-mask: 0.1
mathjax: true
date: 2020-03-07 17:32 
lang: ch 
catalog: true 
categories: documentation
tags:
  - unsupervised learning
  - generative model
  - Time 2020
---

## Introduction
高斯混合模型(Gaussian Mixed Model,GMM)是一种聚类模型，更是一种能够描述数据分布的概率模型。本文在[这篇博客](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)内容的基础上进行翻译、整理，并结合自己的一点理解，旨在便于知识回忆。


## Definition 
高斯混合模型是具备如下概率分布的模型，$\alpha_k$是系数，$\phi(y; \theta_k)$是高斯(正态)分布密度，$\theta_k=(\mu_k,\sigma_k^2)$：

$$
P(y; \theta)=\sum_{k=1}^K\alpha_k \phi(y; \theta_k); \quad \alpha_k\geq 0,\sum_{k=1}^K\alpha_k=1 \\
\phi\left(y; \theta_{k}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left(-\frac{\left(y-\mu_{k}\right)^{2}}{2 \sigma_{k}^{2}}\right)
$$

## GMM with EM
给定观测数据样本点，对其建立、求解高斯混合模型的参数的过程是一种含有隐变量的最大似然参数估计方法，使用[EM算法](/documentation/2020/03/06/post-em/)可以相对有效地解决此问题。

**What is the gmm's latent variable** <br>
假设观测数据为$y_1,y_2,\cdots,y_N$，高斯混合模型为$$P(y\mid \theta)=\sum_{k=1}^K \alpha_k \phi(y\mid \theta_k)$$，其中$\theta=(\alpha_1,\alpha_2,\cdots,\alpha_K;\theta_1,\theta_2,\cdots,\theta_K)$。我们主要使用EM算法估计GMM参数$\theta$。

`latent var`在于观测数据中的一个样本$y_j,\ j=1,2,\cdots,N$不能能够确定是由$K$个高斯分布中的哪一个分布产生的。下面通过数学方式给出定义：

$$
\gamma_{jk}=\left\{\begin{array}{ll}
1, & \text { 第j个观测来自第k个gussian分模型 } \\
0, & \text { 否则 }
\end{array}\right.
$$

上式中$j=1,2,\cdots,N;\quad k=1,2,\cdots,K$，且$\gamma_{jk}$是0-1随机变量。

**EM for GMM** <br>
`#1` init 取各个参数的初始值开始迭代 <br>
`#2` E-step 计算模型$k$对观测数据$y_{j}$的相应度 

$$
\hat{\gamma}_{jk}=\frac{\alpha_{k} \phi\left(y_{j}; \theta_{k}\right)}{\sum_{k=1}^{K} \alpha_{k} \phi\left(y_{j}; \theta_{k}\right)}, \quad j=1,2, \cdots, N ; \quad k=1,2, \cdots, K
$$

`#3` M-step 计算新一轮迭代的模型参数

$$
\begin{array}{c}
\hat{\mu}_{k}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k} y_{j}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}, \quad k=1,2, \cdots, K \\
\hat{\sigma}_{k}^{2}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}\left(y_{j}-\mu_{k}\right)^{2}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}, \quad k=1,2, \cdots, K \\
\hat{\alpha}_{k}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}}{N}, \quad k=1,2, \cdots, K
\end{array}
$$

## Scikit-learn GMM
### Param covariance
<center><img src="/img/in-post/gmm/covariance.pdf" width="80%"></center>

**diag** 这种方式可以独立设置沿每个维度的类蔟大小，并将得到的椭圆约束为与轴对齐 <br>
**spherical** 这种方式约束了类簇的形状，使得所有维度都相等。尽管两种方法并不完全等效，其产生的聚类将具有与kmeans相似的特征 <br>
**full** 这种方式允许将每个簇建模为具有任意方向的椭圆，将得到最复杂且计算量最大的模型。

<span id="component_return"></span>

### Param n_component 
<center><img src="/img/in-post/gmm/n_component.pdf" width="80%"></center>

绘制上述图像的代码详见：[component code](#component)。

<span id="aic_bic_return"></span>

### Set n_component
<center><img src="/img/in-post/gmm/aic_bic.pdf" width="40%"></center>

为了确定GMM模型中的n_component参数，可以采用交叉验证帮助确定参数，一定程度上可以避免过度拟合。除此之外，可以使用一些分析标准来调整模型，如[Akaike information criterion(AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) 或 [Bayesian information criterion(BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion)。<br>
绘制上述图像的代码详见：[aic_bic code](#aic_bic)。

<span id="digits_return"></span>

### As a generator
<center><img src="/img/in-post/gmm/digits.pdf" width="30%"></center>
绘制上述图像的代码详见：[digits code](#digits)。

## GMM vs Kmeans 
构建GMM模型的主要原因是，能够弥补[Kmeans](/documentation/2020/03/07/post-kmeans/)算法的两个主要的缺点：聚类形状的不够灵活和缺少聚类分配的概率值。

聚类形状不灵活：kmeans模型首先在每个cluster的中心放置了一个圆(或者超球)，其半径由聚类中最远的点确定。该半径充当训练集中cluster划分的一个硬截断：任何圆外的数据点不被视为该类的成员；那么聚类的形状只能受限于圆、球、超球，对于一些较为复杂的数据难以正确建模。

缺少聚类分配的概率值，即kmeans聚类的结果只包含两种情况：样本是某一类，或者样本不是某一类，而使用GMM模型能够对于样本的归属进行更加细致的加权。
<span id="kmeans_return"></span>

**Kmeans**
<center><img src="/img/in-post/gmm/kmeans.pdf" width="100%"></center>

第三幅图展示了kmeans cluster会出现的问题。[主成分分析方法](/documentation/2020/03/08/post-pca/)可以解决图中的问题，但是使用PCA在实际中更复杂的问题中，不能完全弥补kmeans上述缺陷。<br>
绘制上述图像的代码详见：[kmeans code](#kmeans)。

<span id="gmm_return"></span>

**GMM**
<center><img src="/img/in-post/gmm/gmm.pdf" width="80%"></center>
如上图所示，充分展示了GMM的数据拟合能力，无论是原始的易分数据还是经过延展的延展数据，GMM模型都能够很好的对于数据聚类模式进行拟合。
绘制上述图像的代码详见：[gmm code](#gmm)。

## Code
<span id="kmeans"></span>

### kmeans code 
```python
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np
from sklearn.datasets.samples_generator import make_blobs

# generate some data
X, y_true = make_blobs(n_samples=400, centers=4, cluster_std=0.60, random_state=0)
X = X[:, ::-1] # flip axes for better plotting

# plot the data with kmeans color-labels
from sklearn.cluster import KMeans
kmeans = KMeans(4, random_state=0)
labels = kmeans.fit(X).predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')

# kmeans circles to separate 4 clusters 
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist

def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):
    labels = kmeans.fit_predict(X)
    # plot the input data
    ax = ax or plt.gca()
    ax.axis('equal')
    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)
    # plot the representation of the KMeans model
    centers = kmeans.cluster_centers_
    radii = [cdist(X[labels == i], [center]).max() for i, center in enumerate(centers)]
    for c, r in zip(centers, radii):
        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))

kmeans = KMeans(n_clusters=4, random_state=0)
plot_kmeans(kmeans, X)

# triky problem using kmeans cluster
rng = np.random.RandomState(13)
X_stretched = np.dot(X, rng.randn(2, 2))
kmeans = KMeans(n_clusters=4, random_state=0)
plot_kmeans(kmeans, X_stretched)
```
[return](#kmeans_return)

<span id="gmm"></span>

### GMM code
```python
from matplotlib.patches import Ellipse

def draw_ellipse(position, covariance, ax=None, **kwargs):
    """Draw an ellipse with a given position and covariance"""
    ax = ax or plt.gca()
    # Convert covariance to principal axes
    if covariance.shape == (2, 2):
        U, s, Vt = np.linalg.svd(covariance)
        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))
        width, height = 2 * np.sqrt(s)
    else:
        angle = 0
        width, height = 2 * np.sqrt(covariance)
    # Draw the Ellipse
    for nsig in range(1, 4):
        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs))
        
def plot_gmm(gmm, X, label=True, ax=None):
    ax = ax or plt.gca()
    labels = gmm.fit(X).predict(X)
    if label:
        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)
    else:
        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)
    ax.axis('equal')
    w_factor = 0.2 / gmm.weights_.max()
    for pos, covar, w in zip(gmm.means_, gmm.covars_, gmm.weights_):
        draw_ellipse(pos, covar, alpha=w * w_factor)

# GMM for original data
gmm = GMM(n_components=4, random_state=42)
plot_gmm(gmm, X)
# GMM for stretched data
gmm = GMM(n_components=4, covariance_type='full', random_state=42)
plot_gmm(gmm, X_stretched)
```
[return](#gmm_return)

<span id="component"></span>

### component code
```python
# raw data
from sklearn.datasets import make_moons
Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)
plt.scatter(Xmoon[:, 0], Xmoon[:, 1])
# n_components=2
gmm2 = GMM(n_components=2, covariance_type='full', random_state=0)
plot_gmm(gmm2, Xmoon)
# n_components=16 
gmm16 = GMM(n_components=16, covariance_type='full', random_state=0)
plot_gmm(gmm16, Xmoon, label=False)
```
[return](#component_return)

<span id="aic_bic"></span>

### aic_bic code
```python
n_components = np.arange(1, 21)
models = [GMM(n, covariance_type='full', random_state=0).fit(Xmoon) for n in n_components]
plt.plot(n_components, [m.bic(Xmoon) for m in models], label='BIC') # BIC
plt.plot(n_components, [m.aic(Xmoon) for m in models], label='AIC') # AIC
plt.legend(loc='best')
plt.xlabel('n_components')
```
[return](#aic_bic_return)

<span id="digits"></span>

### digits code
```python
from sklearn.datasets import load_digits
digits = load_digits() # digits.data.shape=(1797,64)

# plot raw digits data
def plot_digits(data):
    fig, ax = plt.subplots(10, 10, figsize=(8, 8),
                           subplot_kw=dict(xticks=[], yticks=[]))
    fig.subplots_adjust(hspace=0.05, wspace=0.05)
    for i, axi in enumerate(ax.flat):
        im = axi.imshow(data[i].reshape(8, 8), cmap='binary')
        im.set_clim(0, 16)
plot_digits(digits.data)

# pca to preprocess 
from sklearn.decomposition import PCA
pca = PCA(0.99, whiten=True)
data = pca.fit_transform(digits.data) # data.shape=(1797,41)
# set n_component
n_components = np.arange(50, 210, 10)
models = [GMM(n, covariance_type='full', random_state=0)
          for n in n_components]
aics = [model.fit(data).aic(data) for model in models]
plt.plot(n_components, aics)
# generate
gmm = GMM(110, covariance_type='full', random_state=0)
gmm.fit(data)
data_new = gmm.sample(100, random_state=0) # data_new.shape=(100,41)
# PCA to visualize digits
digits_new = pca.inverse_transform(data_new)
plot_digits(digits_new)
```
[return](#digits_return)

## Reference
> 李航 - 统计学习方法 <br>
> https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html

> 1 当使用inline数学公式且公式经过GFM排版之后都在同一行 使用`$...$`符号<br>
> 2 当希望数学公式单独成行或者经过GFM排版之后占用多行 应当使用`$$...$$`符号<br>
> 3 对于表示条件概率 需要表示竖线的时候`|` 应当使用`\mid` 而不是直接在键盘上打出`|` => 容易被编辑器认为是一个md制表符<br>
> 4 在md引入图片的时候 不要使用`<center>`和`</center>` 在这篇文档的编辑过程中vscode的preview插件在使用了上述符号之后 导致下一段的数学公式预览显示不正常<br>
> 5 使用md的时候 单独的两段文字上下需要空出一行<br>
> 6 想要强制换行的时候 需要使用`<br>`而不是`<enter>`<br>
> 7 特殊字符如果想要避免和md解析关键字冲突 应当使用``将关键字包含在内