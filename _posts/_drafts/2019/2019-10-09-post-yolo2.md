---
layout: post
title: "YOLO v2"
subtitle: '结合原文分析|loss function分析'
author: "twistfatezz"
header-style: text
# header-img: "img/post-bg-alitrip.jpg"
# header-mask: 0.1
mathjax: true
date: 2019-10-09 22:36 
lang: ch 
catalog: true 
categories: documentation
tags:
  - object detection
  - Time 2019
---

## Introduction
yolo v2的全名叫做`YOLO9000: Better, Faster, Stronger`，整篇文章通过种种方式，对于yolo v1进行改进，在保持了速度的优势的情况下，精度上得以提升，VOC 2007数据集测试，67FPS下mAP达到76.8%，40FPS下mAP达到78.6%，基本上接近Faster R-CNN和SSD。除此之外，作者提出了一种目标分类与检测的联合训练方法，通过这种方法，yolo 2可以同时在COCO和ImageNet数据集中进行训练，训练后的模型可以实现多达9000种物体的实时检测，因此又名yolo9000。

yolo v2基本是在[yolo v1](/documentation/2019/10/08/post-yolo/)的基础上进行改进。论文中通过一个改进-性能提升总览图对于yolo v2的提升效果进行展示，同时文章也按照这个顺序进行展开介绍。搬运论文中原图如下：

<center><img src="/img/in-post/yolo_v2/v1tov2.png" width="90%"></center>

关于从yolo v1到yolo v2的转变，论文中已经描述的很详细，因此在下面的部分进行简要的概述。

## Improvements 
「**Batch Normalization**」[batch_normalization](/documentation/2020/02/12/batch-normalization/)是一种神经网络正则化的方式，主要解决了深层次神经网络难以训练，梯度弥散的问题。yolo v2在每层卷积层的后面添加了bn层，通过这个做法，成功使得mAP提高了2%。同时，bn是一种网络正则化的方式，在舍弃掉dropout之后可以一定程度上避免过拟合。

「**High Resolution Classiier**」目前的目标检测方法中，基本上都会使用ImageNet预训练过的模型(classifier)来提取特征，如果用的是AlexNet网络，那么输入图片会被resize到不足256 * 256，导致分辨率不够高，给检测带来困难。为此，yolo v2模型把分辨率直接提升到了448*448，这也意味之原有的网络模型必须进行某种调整以适应新的分辨率输入。

对于yolo v2，作者首先对分类网络(自定义的darknet)进行了finetune，分辨率改成448*448，在ImageNet数据集上训练10个epochs，训练后的网络就可以适应高分辨率的输入了。然后，作者对检测网络部分(也就是后半部分)也进行fine tune。这样通过提升输入的分辨率，mAP获得了4%的提升。

「**Convolutional With Anchor Boxes**」作者在这一部分的改进中，主要对于[yolo v1](/documentation/2019/10/08/post-yolo/)中的全连接层替换成了卷积层，以及使用anchor boxes的思想代替了v1中的直接坐标回归的方式。

网络结构的改进：从yolo v1中删除完全连接的图层。下面翻译自原文：首先，我们消除了一个池化层，以使网络卷积层的输出具有更高的分辨率。我们还将网络缩小为可处理416个输入图像，而不是448×448。我们之所以这样做，是因为我们希望feature map中的位置数量为奇数，因此只有一个中心单元。对象(尤其是大对象)往往占据图像的中心，因此最好在中心位置使用一个位置来预测这些对象，而不要使用附近的四个位置。yolo v2的卷积层将图像的图像缩小32倍，因此使用416的输入图像，我们得到13×13的输出特征图。

为什么要使用[faster-rcnn](/documentation/2019/10/06/post-faster-rcnn/)中的anchor机制? yolo v1只能预测98个bbox($$7*7*2$$)，使用anchor之后会，yolo v2可以预测1000($$13*13*9$$)个以上。作者在实验中证实：使用anchor虽然准确率从69.5mAP下降到了69.2mAP，但是recall从81%增加到了88%。

「**Dimension Clusters**」作者在anchor预设尺度上也做了精心的实验，通过对于训练数据集进行ground truth bbox尺度的k-means聚类为bbox的尺度选择了最好的先验。作者没有直接使用欧式距离作为聚类的标准，因为最大的bbox产生的误差通常会比较小的盒子产生的误差大，而实际上，较小的盒子可能iou偏差更大。为了解决这个问题，作者采用如下指标作为k-means的距离度量：

$$
d(\text { box, centroid })=1-\operatorname{IOU}(\text { box, centroid })
$$

即和聚类中心距离的bbox的iou越大，距离越近。如上面的折线图所示，平均iou反映了选定k个cluster中心的bbox尺寸之后，能够以多大程度`代表`整个数据集中的bbox尺度。显然，选定的聚类中心越多，平均iou越大。

「**Direct location prediction**」作者在yolo v2中继续采用了yolo v1中的直接进行bbox相对**指定cell左上角**回归预测的方式，在v2的论文中提到，使用RCNN系列的offset进行回归预测的方法导致了训练初期模型非常不稳定，作者的解释是：根据RCNN系列的bbox regression的公式(详见[RCNN](/documentation/2019/09/29/post-rcnn/)文章整理)，如果$t_x'=1$则相当于$x_{finetune}-x_{proposal}=w_{proposal}$，意味着要求网络生成的bbox相比预设值偏离有整个预设bbox的宽度的距离!这样大的bbox位置的调整在yolo v2的作者看来是导致了训练初期模型不稳定的因素。因此，yolo v2作者的想法是，每一个cell的预测中心就在这个cell里面，不进行大的移动，可以让模型快速收敛。yolo v2中最后使用的bbox regression的方式是(通过$\sigma$函数将网络输出数值映射到01之间)：

$$
b_{x}=\sigma\left(t_{x}\right)+c_{x}, \quad b_{y}=\sigma\left(t_{y}\right)+c_{y}
$$

$$
b_{w}=p_{w} e^{t_{w}},\quad b_{h}=p_{h} e^{t_{h}} 
$$

$$
\operatorname{Pr}(\text {object}) \times \operatorname{IOU}(b, \text {object})=\sigma\left(t_{o}\right) 
$$

注意这里的bbox中心坐标的回归使用了相对cell左上角偏移的方式，并且通过$\sigma$函数将中心的回归限制在cell内部；而bbox的长宽的变换的回归采用了[RCNN](/2019/09/29/post-rcnn/)中的方式。另外输出的$\sigma(t_o)$来自[yolo v1](/documentation/2019/10/08/post-yolo/)中的`bbox_confidence`。

「**Fine-Grained Features**」作者受到[Faster-RCNN](/documentation/2019/10/06/post-faster-rcnn/)和[SSD](/documentation/2019/10/04/post-ssd/)的启发，认为适当从多种尺度特征图中提取关于物体的信息会增加检测的准确度，在最后的层引入跨层连接，称之为`passthrough layer`。但是和它们不同的是，作者使用了一种类似于Resnet的跨层连接实现了不同深度特征图的特征融合，模拟多尺度特征提取效果，最终值得最终的结果有1%的性能提升。关于如何引入的跨层连接详见论文中darknet-19结构图(图片来自[这个博客](https://luckmoonlight.github.io/2018/11/28/yoloV1yolov2yoloV3/))：

<center><img src="/img/in-post/yolo_v2/structure.png" width="60%"></center>

「**Multi-Scale Training**」yolo v2的结构中仅包含卷积和池化层，所以能够接受不同尺度的图像输入，在训练的过程中不固定输入图像的大小，每隔几次迭代就更改网络输入尺度。以下引自原文：每10epoch，我们的网络就会随机选择一个新的图像尺寸。由于我们的模型下采样了32倍(从输入图像到用于detection的feature map尺度计算得到416/32=13)，因此我们从32的倍数中选取输入图像尺寸(能够整除)：(320，352，...，608)。因此，最小的选择是320×320，最大的选择是608×608。我们将网络调整为该尺寸，然后继续训练。

「**YOLOv2速度的改进(Faster)**」yolo一向是速度和精度并重，作者为了改善检测速度，也作了一些相关工作。大多数检测网络有赖于VGG-16作为特征提取部分，VGG-16的确是一个强大而准确的分类网络，但是复杂度有些冗余。224*224的图片进行一次前向传播，其卷积层就需要多达306.9亿次浮点数运算。yolo v2使用的是基于Googlenet改造的darknet-19网络，比VGG-16更快，一次前向传播仅需85.2亿次运算。可是它的精度要略低于VGG-16，单张224x224取前五个预测概率的对比成绩为88%和90%(低一点点也是可以接受)。

「**Training for classification**」

作者使用Darknet-19在标准1000类的ImageNet上训练了160次，用的随机梯度下降法，starting learning rate 为0.1，polynomial rate decay 为4，weight decay为0.0005，momentum 为0.9。训练的时候仍然使用了很多常见的数据扩充方法(data augmentation)，包括random crops, rotations, and hue, saturation, and exposure shifts。(这些训练参数是基于darknet框架，和caffe不尽相同)

初始的224*224训练后，作者把分辨率上调到了448x448，然后又训练了10次，学习率调整到了0.001。高分辨率下训练的分类网络在top-1准确率76.5%，top-5准确率93.3%。

「**Training for detection**」

分类网络训练完后，就该训练检测网络了，作者去掉了原网络最后一个卷积层，转而增加了三个3x3x1024的卷积层(可参考darknet中cfg文件)，并且在每一个上述卷积层后面跟一个1x1的卷积层，输出维度是检测所需的数量。对于VOC数据集，预测5种boxes大小，每个box包含5个坐标值和20个类别，所以总共是5x(5+20)=125个输出维度。同时也添加了转移层(passthrough layer)，从最后那个3x3x512的卷积层连到倒数第二层，使模型有了细粒度特征。

作者的检测模型以0.001的初始学习率训练了160次，在60次和90次的时候，学习率减为原来的十分之一。其他的方面，weight decay为0.0005，momentum为0.9，依然使用了类似于Faster-RCNN和SSD的数据扩充(data augmentation)策略。

「**yolov2分类的改进(Stronger)**」
这一部分，作者使用联合训练方法，结合wordtree等方法，使yolo v2的检测种类扩充到了上千种，具体内容待续。

## Loss Function
在介绍yolo v2的loss function之前首先需要对于loss function中的指示函数的定义有所了解。注意，仔细阅读yolo v2论文，你会发现，yolo v1中提到的`the grid cell is resiponsible for detecting that object`，的概念已经没有再提到过了，这是因为v1的这种`cell_to_object`方式不能处理两个物体中心位于同一个cell内部的情况，而且整个预测的模型机制简单，变得只有速度，没有性能。因此v2中采用了`anchor_to_object`的对应方式，解决了yolo v1的上述问题。下面给出v2中loss function的指示函数的定义：

$$1_{ij}^{responsible_{-}obj}$$表示`正样本`，具体来说，每个gt bbox的中心位置的小格子被选择以用于预测，被选出的小格子们中都有5个不同尺度的prior bbox(来源于kmeans聚类)，将每个小格子中的每个prior bbox和这个输入图像中的所有的gt bbox计算iou，取iou最大那个prior bbox对应的pred bbox用于相应的gt bbox的回归预测，叫作`responsible obj`。补充：每个物体的中心只能位于一个小格子里面，每个小格子中的5个prior bbox和这个物体iou最大的只有一个，这就保证了`anchor_to_object`的一一对应性，但是如果anchor太少，仍然有可能出现多个重合度高的gt对应一个anchor，我觉得这也是yolo v3增加anchor bbox到9个的原因。

$$1_{ij}^{no_{-}responsible_{-}obj}$$表示`负样本`，具体来说，需要先计算每个小格子中的每个预测框和所有ground truth bbox(yolo v2中默认每个图像中至多考虑30个gt物体)的iou值，并且取最大值，如果该值小于一定的阈值(yolo v2使用的是0.6)，那么这个预测框就标记为background，也就是noobj。那么是否可以认为除了`正样本`之外都是`负样本`么？不是的，正负样本之间还有一些样本不计入所有的loss中，可以称之为`无关样本`，即那些能够和某个gt bbox之间的iou大于上述阈值，但是有够不上`responsible_obj`的标准的那些anchors。

本文的最后贴出了yolo v2官方的loss function的源代码，作者根据自己的理解，进行了**粗粒度**的解析，将每部分代码和下面介绍的loss function的部分对应起来(在代码中有相应注释)。另外，作者对于网上常见的两个版本的loss function写法(两个版本是同一个东西的不同的写法)进行归纳，将相同的loss部分对应在下面。


### Part 1
「**用于预测物体的anchor的坐标回归loss**」yolo v2的loss中将yolo v1中的坐标回归中的`开根号`换回了平方(MSE)的形式。个人认为采用smooth-L1-loss可能更好，MSE对于异常数值太过敏感。

第一个版本：

$$
\lambda_{obj}^{coord} \sum_{i}^{S^{2}} \sum_{j}^{B} 1_{i j}^{responsible_{-}obj}\left(x_{ij}^{pred}-x_{ij}^{o b j}\right)^{2}+\left(y_{ij}^{pred}-y_{ij}^{obj}\right)^{2}+\left(w_{i j}^{pred}-w_{ij}^{obj}\right)^{2}+\left(h_{i j}^{pred}-h_{ij}^{obj}\right)^{2}
$$

第二个版本：

$$
+\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} 1_k^{truth}  \lambda_{\text {coord}} * \sum_{r \epsilon(x, y, w, h)}\left(\operatorname{truth}^{r}-b_{ijk}^{r}\right)^{2}
$$

### Part 2
「**不用于预测物体的anchor的坐标回归**」你可能会有疑问，不用做物体位置预测，为什么还要回归，回归到什么位置呢？其实，这部分loss是anchor回归训练中的一个技巧，相当于网络预测bbox的一种`预热`手段。真实的图像中任何位置都有可能出现所谓的gt bbox，所以，在iteration samples<12800的时候加入这部分loss可以使得不负责预测的anchor能够将其预测值回归到各自的prior bbox，这样一来，如果新的位置(从来没有被训练过的anchor)得到了一个gt bbox供其回归的时候，它能够以一个较小的`学习成本`进行学习，这样算法也易于收敛。其实，yolo v2中多次使用了这个思想：这个做法和使用k-means聚类做数据anchor bbox size先验的思想如出一辙。

第一个版本：

$$
+\lambda_{noobj}^{coord} \sum_{i}^{S^2} \sum_{j}^{B} 1_{i j}^{no_{-} \text {responsible}_{-} obj}\left(x_{i j}^{\text {pred}}-x_{i j}^{\text {anchor}_{-} \text {center}}\right)^{2}+\left(y_{i j}^{\text {pred}}-y_{i j}^{\text {anchor}_{-} \text {center}}\right)^{2}
+\left(w_{i j}^{pred}-w_{ij}^{anchor\_default} \right)^{2}+\left(h_{ij}^{\text {pred}}-h_{i j}^{\text {anchor}_{-} \text {default}}\right)^{2}
$$

第二个版本：

$$
+1_{t<12800} \lambda_{\text {prior}} * \sum_{r \in(x, y, w, h)}\left(\text {prior}_{k}^{r}-b_{i j k}^{r}\right)^{2}
$$

### Part 3
[**bbox confidence中obj损失**]
`responsible obj`的定义如下：对于某个ground truth，首先要确定其中心点要落在哪个cell上，然后计算这个cell的5个先验框与ground truth的iou值(yolo v2中bias_match=1)，计算iou值时不考虑坐标，只考虑形状，所以先将先验框与ground truth的中心点都偏移到同一位置(原点)，然后计算出对应的iou值，iou值最大的那个先验框与ground truth匹配，对应的预测框用来预测这个ground truth。**总结一下就是先验框用于判断cell以及对应的pred bbox和gt bbox是否匹配，并且负责pred bbox初始化，pred bbox用来回归到gt bbox用于预测。**
下面公式用于计算负责预测gt的bbox confidence损失。直接减去iou的原因参考[yolo v1](/documentation/2019/10/08/post-yolo/)中的`mask`损失中$P_r(object)=1$就是label。

第一个版本：

$$
+\lambda_{obj}^{conf} \sum_{i}^{S^{2}} \sum_{j}^{B} 1_{i j}^{responsible_{-} obj} \left (\operatorname{conf}_{i j}^{pred}-iou(box_{i j}^{pred}, box_{i j}^{trut h})\right )^{2}
$$

第二个版本：

$$
+\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} 1_k^{truth} \lambda_{obj} *\left(IOU_{truth}^{k}-b_{ijk}^{o}\right)^{2}
$$

### Part 4
[**bbox confidence中noobj损失**]
`no responsible obj`的定义如下：首先计算每个小格子中的每个anchor和所有gt bbox的iou，如果算出来的最大iou<threshold，则相应的$\lambda_{noobj}^{conf}=1$，并且confidence的label就是0，也就是说这个anchor是属于`noobj`。但是，如果这个值大于threhold，相应的$\lambda_{noobj}^{conf}=0$。简单解释一下：对于不负责预测物体的小格子，如果这个小格子中的预测出来的bbox和gt_bbox的iou小于0.6，则将这个预测的bbox_confidence作为负样本(bbox中没有物体)加入loss中，否则($iou>0.6$)不作为noobj部分进行考虑($\lambda =0$)。

第一个版本写法：

$$
+\lambda_{noobj}^{conf} \sum_{i}^{S^{2}} \sum_{j}^{B} 1_{ij}^{no_{-}responsible_{-}obj} \left(conf_{i j}^{pred}-0 \right)^{2}
$$

第二个版本的写法：

$$
+\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} \quad 1_{\operatorname{Max} IOU<\text {Thresh}} \lambda_{\text {noobj}} *\left(-b_{i j k}^{o}\right)^{2}
$$

### Part 5
「**obj classification损失**」
注意这个公式和[yolo v1](/documentation/2019/10/08/post-yolo/)中的grid_confidence不一样，v1中一个小格子只能对应一个物体(类别)，而v2中，每一个小格子中的一个bbox对应一个类别回归，因此v2中每个划分的小格子可以同时预测不同类别的物体。

第一个版本：

$$
+\sum_{i}^{S^{2}} \sum_{j}^{B} 1_{ij}^{\text {responsible}_{-} obj} \left (p_{ij}^{\text {pred}}(c)-p_{ij}^{\text {truth}}(c) \right) ^{2}
$$

第二个版本：

$$
+\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} 1_k^{truth} \lambda_{\text {class}} * \sum_{c=1}^{c}\left(\operatorname{truth}^{c}-b_{ijk}^{c}\right)^{2}
$$

yolo中的匹配机制和[SSD](/documentation/2019/10/04/post-ssd/)与[faster-rcnn](/documentation/2019/10/06/post-faster-rcnn/)中的RPN网络的处理方式有很大不同，yolo对于一个ground truth只选取一个cell(v1)或者一个anchor(v2)进行处理，而后面两个框架将一个ground truth分配给多个先验框。
## Code
代码参考自https://github.com/pjreddie/darknet/blob/master/src/region_layer.c
```c++
#include "region_layer.h"
#include "activations.h"
#include "blas.h"
#include "box.h"
#include "cuda.h"
#include "utils.h"

#include <stdio.h>
#include <assert.h>
#include <string.h>
#include <stdlib.h>

layer make_region_layer(int batch, int w, int h, int n, int classes, int coords)
{
    layer l = {0};
    l.type = REGION;

    l.n = n;
    l.batch = batch;
    l.h = h;
    l.w = w;
    l.c = n*(classes + coords + 1);
    l.out_w = l.w;
    l.out_h = l.h;
    l.out_c = l.c;
    l.classes = classes;
    l.coords = coords;
    l.cost = calloc(1, sizeof(float));
    l.biases = calloc(n*2, sizeof(float));
    l.bias_updates = calloc(n*2, sizeof(float));
    l.outputs = h*w*n*(classes + coords + 1);
    l.inputs = l.outputs;
    l.truths = 30*(l.coords + 1);
    l.delta = calloc(batch*l.outputs, sizeof(float));
    l.output = calloc(batch*l.outputs, sizeof(float));
    int i;
    for(i = 0; i < n*2; ++i){
        l.biases[i] = .5;
    }

    l.forward = forward_region_layer;
    l.backward = backward_region_layer;
#ifdef GPU
    l.forward_gpu = forward_region_layer_gpu;
    l.backward_gpu = backward_region_layer_gpu;
    l.output_gpu = cuda_make_array(l.output, batch*l.outputs);
    l.delta_gpu = cuda_make_array(l.delta, batch*l.outputs);
#endif

    fprintf(stderr, "detection\n");
    srand(0);

    return l;
}

void resize_region_layer(layer *l, int w, int h)
{
    l->w = w;
    l->h = h;

    l->outputs = h*w*l->n*(l->classes + l->coords + 1);
    l->inputs = l->outputs;

    l->output = realloc(l->output, l->batch*l->outputs*sizeof(float));
    l->delta = realloc(l->delta, l->batch*l->outputs*sizeof(float));

#ifdef GPU
    cuda_free(l->delta_gpu);
    cuda_free(l->output_gpu);

    l->delta_gpu =     cuda_make_array(l->delta, l->batch*l->outputs);
    l->output_gpu =    cuda_make_array(l->output, l->batch*l->outputs);
#endif
}

// get bbox from predict values
box get_region_box(float *x, float *biases, int n, int index, int i, int j, int w, int h, int stride)
{
    box b;
    b.x = (i + x[index + 0*stride]) / w;
    b.y = (j + x[index + 1*stride]) / h;
    b.w = exp(x[index + 2*stride]) * biases[2*n]   / w;
    b.h = exp(x[index + 3*stride]) * biases[2*n+1] / h;
    return b;
}

// return iou | regist delta matrix
float delta_region_box(box truth, float *x, float *biases, int n, int index, int i, int j, int w, int h, float *delta, float scale, int stride)
{
    box pred = get_region_box(x, biases, n, index, i, j, w, h, stride);
    float iou = box_iou(pred, truth);

    float tx = (truth.x*w - i);
    float ty = (truth.y*h - j);
    float tw = log(truth.w*w / biases[2*n]);
    float th = log(truth.h*h / biases[2*n + 1]);

    delta[index + 0*stride] = scale * (tx - x[index + 0*stride]);
    delta[index + 1*stride] = scale * (ty - x[index + 1*stride]);
    delta[index + 2*stride] = scale * (tw - x[index + 2*stride]);
    delta[index + 3*stride] = scale * (th - x[index + 3*stride]);
    return iou;
}

void delta_region_mask(float *truth, float *x, int n, int index, float *delta, int stride, int scale)
{
    int i;
    for(i = 0; i < n; ++i){
        delta[index + i*stride] = scale*(truth[i] - x[index + i*stride]);
    }
}


void delta_region_class(float *output, float *delta, int index, int class, int classes, tree *hier, float scale, int stride, float *avg_cat, int tag)
{
    int i, n;
    if(hier){
        float pred = 1;
        while(class >= 0){
            pred *= output[index + stride*class];
            int g = hier->group[class];
            int offset = hier->group_offset[g];
            for(i = 0; i < hier->group_size[g]; ++i){
                delta[index + stride*(offset + i)] = scale * (0 - output[index + stride*(offset + i)]);
            }
            delta[index + stride*class] = scale * (1 - output[index + stride*class]);

            class = hier->parent[class];
        }
        *avg_cat += pred;
    } else {
        if (delta[index] && tag){
            delta[index + stride*class] = scale * (1 - output[index + stride*class]);
            return;
        }
        for(n = 0; n < classes; ++n){
            delta[index + stride*n] = scale * (((n == class)?1 : 0) - output[index + stride*n]);
            if(n == class) *avg_cat += output[index + stride*n];
        }
    }
}

float logit(float x)
{
    return log(x/(1.-x));
}

float tisnan(float x){
    return (x != x);
}

int entry_index(layer l, int batch, int location, int entry){
    int n = location / (l.w*l.h);
    int loc = location % (l.w*l.h);
    return batch*l.outputs + n*l.w*l.h*(l.coords+l.classes+1) + entry*l.w*l.h + loc;
}

void forward_region_layer(const layer l, network net)
{
    int i,j,b,t,n;
    memcpy(l.output, net.input, l.outputs*l.batch*sizeof(float));

#ifndef GPU
    for (b = 0; b < l.batch; ++b){
        for(n = 0; n < l.n; ++n){
            int index = entry_index(l, b, n*l.w*l.h, 0);
            activate_array(l.output + index, 2*l.w*l.h, LOGISTIC);
            index = entry_index(l, b, n*l.w*l.h, l.coords);
            if(!l.background) activate_array(l.output + index,   l.w*l.h, LOGISTIC);
            index = entry_index(l, b, n*l.w*l.h, l.coords + 1);
            if(!l.softmax && !l.softmax_tree) activate_array(l.output + index, l.classes*l.w*l.h, LOGISTIC);
        }
    }
    if (l.softmax_tree){
        int i;
        int count = l.coords + 1;
        for (i = 0; i < l.softmax_tree->groups; ++i) {
            int group_size = l.softmax_tree->group_size[i];
            softmax_cpu(net.input + count, group_size, l.batch, l.inputs, l.n*l.w*l.h, 1, l.n*l.w*l.h, l.temperature, l.output + count);
            count += group_size;
        }
    } else if (l.softmax){
        int index = entry_index(l, 0, 0, l.coords + !l.background);
        softmax_cpu(net.input + index, l.classes + l.background, l.batch*l.n, l.inputs/l.n, l.w*l.h, 1, l.w*l.h, 1, l.output + index);
    }
#endif

    memset(l.delta, 0, l.outputs * l.batch * sizeof(float));
    if(!net.train) return;
    float avg_iou = 0;
    float recall = 0;
    float avg_cat = 0;
    float avg_obj = 0;
    float avg_anyobj = 0;
    int count = 0;
    int class_count = 0;
    *(l.cost) = 0;
    for (b = 0; b < l.batch; ++b){// for each sample in batch
        // this if is no using
        if(l.softmax_tree){
            int onlyclass = 0;
            for(t = 0; t < 30; ++t){// most has 30 gt bbox in a sample
                box truth = float_to_box(net.truth + t*(l.coords + 1) + b*l.truths, 1);
                if(!truth.x) break;// no gt in the sample
                int class = net.truth[t*(l.coords + 1) + b*l.truths + l.coords];
                float maxp = 0;
                int maxi = 0;
                // skip this if? guess this part is fallen into the circle of yolo v1, so it's no use in yolo v2 here!
                if(truth.x > 100000 && truth.y > 100000){
                    for(n = 0; n < l.n*l.w*l.h; ++n){// for each anchor in a sample's fmap
                        int class_index = entry_index(l, b, n, l.coords + 1);// anchor's class
                        int obj_index = entry_index(l, b, n, l.coords);// anchor's bbox
                        float scale = l.output[obj_index];
                        l.delta[obj_index] = l.noobject_scale * (0 - l.output[obj_index]);// regist noobj bbox confidece loss
                        float p = scale*get_hierarchy_probability(l.output + class_index, l.softmax_tree, class, l.w*l.h);// compute Pr(object)
                        if(p > maxp){
                            maxp = p;// max Pr
                            maxi = n;// related idx
                        }
                    }
                    int class_index = entry_index(l, b, maxi, l.coords + 1);
                    int obj_index = entry_index(l, b, maxi, l.coords);
                    delta_region_class(l.output, l.delta, class_index, class, l.classes, l.softmax_tree, l.class_scale, l.w*l.h, &avg_cat, !l.softmax);
                    if(l.output[obj_index] < .3){l.delta[obj_index] = l.object_scale * (.3 - l.output[obj_index]);}
                    else{l.delta[obj_index] = 0;}
                    l.delta[obj_index] = 0;
                    ++class_count;
                    onlyclass = 1;
                    break;
                }
            }
            if(onlyclass) continue;
        }
        // loss of the noobj bboxes 
        for(j = 0; j < l.h; ++j){// height
            for(i = 0; i < l.w; ++i){// width
                for(n = 0; n < l.n; ++n){// anchor 
                    int box_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, 0);// get pred box index
                    box pred = get_region_box(l.output, l.biases, n, box_index, i, j, l.w, l.h, l.w*l.h);// get pred coordinate
                    // find the pred bbox compute from a given anchor which has a with max iou with gt
                    float best_iou = 0;
                    for(t = 0; t < 30; ++t){
                        box truth = float_to_box(net.truth + t*(l.coords + 1) + b*l.truths, 1);
                        if(!truth.x) break;
                        float iou = box_iou(pred, truth);
                        if(iou > best_iou){
                            best_iou = iou;
                        }
                    }
                    int obj_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, l.coords);
                    avg_anyobj += l.output[obj_index];
                    l.delta[obj_index] = l.noobject_scale * (0 - l.output[obj_index]);// compute noobj bbox confidence loss
                    if(l.background) l.delta[obj_index] = l.noobject_scale * (1 - l.output[obj_index]);
                    if (best_iou > l.thresh){// remove matched bbox confidence loss by setting it as 0
                        l.delta[obj_index] = 0;
                    }
                    // iterations<12800
                    if(*(net.seen) < 12800){
                        // set the anchor bbox center at the cell's center
                        box truth = {0};
                        truth.x = (i + .5)/l.w;
                        truth.y = (j + .5)/l.h;
                        truth.w = l.biases[2*n]/l.w;
                        truth.h = l.biases[2*n+1]/l.h;
                        // regist delta iou loss between anchor bbox and pred bbox
                        delta_region_box(truth, l.output, l.biases, n, box_index, i, j, l.w, l.h, l.delta, .01, l.w*l.h);
                    }
                }
            }
        }
        // loss of the obj bbox(bbox regression + bbox confidence + classification loss)
        for(t = 0; t < 30; ++t){
            box truth = float_to_box(net.truth + t*(l.coords + 1) + b*l.truths, 1);
            if(!truth.x) break;
            float best_iou = 0;
            int best_n = 0;
            i = (truth.x * l.w);
            j = (truth.y * l.h);
            // move tmp gt bbox's left top to (0,0) | only care about the shape diff between the anchor and the gt | neglect the center diff between them
            box truth_shift = truth;
            truth_shift.x = 0;
            truth_shift.y = 0;
            // each cell hold 5 anchor bbox | l.n=5
            for(n = 0; n < l.n; ++n){
                int box_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, 0);
                box pred = get_region_box(l.output, l.biases, n, box_index, i, j, l.w, l.h, l.w*l.h);
                if(l.bias_match){
                    pred.w = l.biases[2*n]/l.w;
                    pred.h = l.biases[2*n+1]/l.h;
                }
                pred.x = 0;
                pred.y = 0;
                // find best pred with best iou with current gt bbox
                float iou = box_iou(pred, truth_shift);
                if (iou > best_iou){
                    best_iou = iou;// get best iou of (gt bbox, anchor) 
                    best_n = n;// get the related anchor idx
                }
            }

            // compute obj bbox regression loss
            int box_index = entry_index(l, b, best_n*l.w*l.h + j*l.w + i, 0);// get the matched anchor related pred bbox idx
            float iou = delta_region_box(truth, l.output, l.biases, best_n, box_index, i, j, l.w, l.h, l.delta, l.coord_scale * (2 - truth.w*truth.h), l.w*l.h);// regist iou loss
            // compute obj confidence loss
            if(l.coords > 4){
                int mask_index = entry_index(l, b, best_n*l.w*l.h + j*l.w + i, 4);// get the matched anchor relatd mask idx
                delta_region_mask(net.truth + t*(l.coords + 1) + b*l.truths + 5, l.output, l.coords - 4, mask_index, l.delta, l.w*l.h, l.mask_scale);
            }
            if(iou > .5) recall += 1;
            avg_iou += iou;

            int obj_index = entry_index(l, b, best_n*l.w*l.h + j*l.w + i, l.coords);
            avg_obj += l.output[obj_index];
            l.delta[obj_index] = l.object_scale * (1 - l.output[obj_index]);
            if (l.rescore){// if obj
                l.delta[obj_index] = l.object_scale * (iou - l.output[obj_index]);
            }
            if(l.background){// if noobj
                l.delta[obj_index] = l.object_scale * (0 - l.output[obj_index]);
            }

            // compute obj classification loss
            int class = net.truth[t*(l.coords + 1) + b*l.truths + l.coords];
            if (l.map) class = l.map[class];
            int class_index = entry_index(l, b, best_n*l.w*l.h + j*l.w + i, l.coords + 1);
            delta_region_class(l.output, l.delta, class_index, class, l.classes, l.softmax_tree, l.class_scale, l.w*l.h, &avg_cat, !l.softmax);
            ++count;
            ++class_count;
        }
    }
    *(l.cost) = pow(mag_array(l.delta, l.outputs * l.batch), 2);
    printf("Region Avg IOU: %f, Class: %f, Obj: %f, No Obj: %f, Avg Recall: %f,  count: %d\n", avg_iou/count, avg_cat/class_count, avg_obj/count, avg_anyobj/(l.w*l.h*l.n*l.batch), recall/count, count);
}

void backward_region_layer(const layer l, network net)
{
    /*
       int b;
       int size = l.coords + l.classes + 1;
       for (b = 0; b < l.batch*l.n; ++b){
       int index = (b*size + 4)*l.w*l.h;
       gradient_array(l.output + index, l.w*l.h, LOGISTIC, l.delta + index);
       }
       axpy_cpu(l.batch*l.inputs, 1, l.delta, 1, net.delta, 1);
     */
}

void correct_region_boxes(detection *dets, int n, int w, int h, int netw, int neth, int relative)
{
    int i;
    int new_w=0;
    int new_h=0;
    if (((float)netw/w) < ((float)neth/h)) {
        new_w = netw;
        new_h = (h * netw)/w;
    } else {
        new_h = neth;
        new_w = (w * neth)/h;
    }
    for (i = 0; i < n; ++i){
        box b = dets[i].bbox;
        b.x =  (b.x - (netw - new_w)/2./netw) / ((float)new_w/netw); 
        b.y =  (b.y - (neth - new_h)/2./neth) / ((float)new_h/neth); 
        b.w *= (float)netw/new_w;
        b.h *= (float)neth/new_h;
        if(!relative){
            b.x *= w;
            b.w *= w;
            b.y *= h;
            b.h *= h;
        }
        dets[i].bbox = b;
    }
}

void get_region_detections(layer l, int w, int h, int netw, int neth, float thresh, int *map, float tree_thresh, int relative, detection *dets)
{
    int i,j,n,z;
    float *predictions = l.output;
    if (l.batch == 2) {
        float *flip = l.output + l.outputs;
        for (j = 0; j < l.h; ++j) {
            for (i = 0; i < l.w/2; ++i) {
                for (n = 0; n < l.n; ++n) {
                    for(z = 0; z < l.classes + l.coords + 1; ++z){
                        int i1 = z*l.w*l.h*l.n + n*l.w*l.h + j*l.w + i;
                        int i2 = z*l.w*l.h*l.n + n*l.w*l.h + j*l.w + (l.w - i - 1);
                        float swap = flip[i1];
                        flip[i1] = flip[i2];
                        flip[i2] = swap;
                        if(z == 0){
                            flip[i1] = -flip[i1];
                            flip[i2] = -flip[i2];
                        }
                    }
                }
            }
        }
        for(i = 0; i < l.outputs; ++i){
            l.output[i] = (l.output[i] + flip[i])/2.;
        }
    }
    for (i = 0; i < l.w*l.h; ++i){
        int row = i / l.w;
        int col = i % l.w;
        for(n = 0; n < l.n; ++n){
            int index = n*l.w*l.h + i;
            for(j = 0; j < l.classes; ++j){
                dets[index].prob[j] = 0;
            }
            int obj_index  = entry_index(l, 0, n*l.w*l.h + i, l.coords);
            int box_index  = entry_index(l, 0, n*l.w*l.h + i, 0);
            int mask_index = entry_index(l, 0, n*l.w*l.h + i, 4);
            float scale = l.background ? 1 : predictions[obj_index];
            dets[index].bbox = get_region_box(predictions, l.biases, n, box_index, col, row, l.w, l.h, l.w*l.h);
            dets[index].objectness = scale > thresh ? scale : 0;
            if(dets[index].mask){
                for(j = 0; j < l.coords - 4; ++j){
                    dets[index].mask[j] = l.output[mask_index + j*l.w*l.h];
                }
            }

            int class_index = entry_index(l, 0, n*l.w*l.h + i, l.coords + !l.background);
            if(l.softmax_tree){

                hierarchy_predictions(predictions + class_index, l.classes, l.softmax_tree, 0, l.w*l.h);
                if(map){
                    for(j = 0; j < 200; ++j){
                        int class_index = entry_index(l, 0, n*l.w*l.h + i, l.coords + 1 + map[j]);
                        float prob = scale*predictions[class_index];
                        dets[index].prob[j] = (prob > thresh) ? prob : 0;
                    }
                } else {
                    int j =  hierarchy_top_prediction(predictions + class_index, l.softmax_tree, tree_thresh, l.w*l.h);
                    dets[index].prob[j] = (scale > thresh) ? scale : 0;
                }
            } else {
                if(dets[index].objectness){
                    for(j = 0; j < l.classes; ++j){
                        int class_index = entry_index(l, 0, n*l.w*l.h + i, l.coords + 1 + j);
                        float prob = scale*predictions[class_index];
                        dets[index].prob[j] = (prob > thresh) ? prob : 0;
                    }
                }
            }
        }
    }
    correct_region_boxes(dets, l.w*l.h*l.n, w, h, netw, neth, relative);
}

#ifdef GPU

void forward_region_layer_gpu(const layer l, network net)
{
    copy_gpu(l.batch*l.inputs, net.input_gpu, 1, l.output_gpu, 1);
    int b, n;
    for (b = 0; b < l.batch; ++b){
        for(n = 0; n < l.n; ++n){
            int index = entry_index(l, b, n*l.w*l.h, 0);
            activate_array_gpu(l.output_gpu + index, 2*l.w*l.h, LOGISTIC);
            if(l.coords > 4){
                index = entry_index(l, b, n*l.w*l.h, 4);
                activate_array_gpu(l.output_gpu + index, (l.coords - 4)*l.w*l.h, LOGISTIC);
            }
            index = entry_index(l, b, n*l.w*l.h, l.coords);
            if(!l.background) activate_array_gpu(l.output_gpu + index,   l.w*l.h, LOGISTIC);
            index = entry_index(l, b, n*l.w*l.h, l.coords + 1);
            if(!l.softmax && !l.softmax_tree) activate_array_gpu(l.output_gpu + index, l.classes*l.w*l.h, LOGISTIC);
        }
    }
    if (l.softmax_tree){
        int index = entry_index(l, 0, 0, l.coords + 1);
        softmax_tree(net.input_gpu + index, l.w*l.h, l.batch*l.n, l.inputs/l.n, 1, l.output_gpu + index, *l.softmax_tree);
    } else if (l.softmax) {
        int index = entry_index(l, 0, 0, l.coords + !l.background);
        softmax_gpu(net.input_gpu + index, l.classes + l.background, l.batch*l.n, l.inputs/l.n, l.w*l.h, 1, l.w*l.h, 1, l.output_gpu + index);
    }
    if(!net.train || l.onlyforward){
        cuda_pull_array(l.output_gpu, l.output, l.batch*l.outputs);
        return;
    }

    cuda_pull_array(l.output_gpu, net.input, l.batch*l.inputs);
    forward_region_layer(l, net);
    //cuda_push_array(l.output_gpu, l.output, l.batch*l.outputs);
    if(!net.train) return;
    cuda_push_array(l.delta_gpu, l.delta, l.batch*l.outputs);
}

void backward_region_layer_gpu(const layer l, network net)
{
    int b, n;
    for (b = 0; b < l.batch; ++b){
        for(n = 0; n < l.n; ++n){
            int index = entry_index(l, b, n*l.w*l.h, 0);
            gradient_array_gpu(l.output_gpu + index, 2*l.w*l.h, LOGISTIC, l.delta_gpu + index);
            if(l.coords > 4){
                index = entry_index(l, b, n*l.w*l.h, 4);
                gradient_array_gpu(l.output_gpu + index, (l.coords - 4)*l.w*l.h, LOGISTIC, l.delta_gpu + index);
            }
            index = entry_index(l, b, n*l.w*l.h, l.coords);
            if(!l.background) gradient_array_gpu(l.output_gpu + index,   l.w*l.h, LOGISTIC, l.delta_gpu + index);
        }
    }
    axpy_gpu(l.batch*l.inputs, 1, l.delta_gpu, 1, net.delta_gpu, 1);
}
#endif

void zero_objectness(layer l)
{
    int i, n;
    for (i = 0; i < l.w*l.h; ++i){
        for(n = 0; n < l.n; ++n){
            int obj_index = entry_index(l, 0, n*l.w*l.h + i, l.coords);
            l.output[obj_index] = 0;
        }
    }
}

```

## Reference
> 部分翻译自原文A <br>
> https://blog.csdn.net/jesse_mx/article/details/53925356 <br>
> https://zhuanlan.zhihu.com/p/40659490 <br>
> https://zhuanlan.zhihu.com/p/35325884

> 1 当使用inline数学公式且公式经过GFM排版之后都在同一行 使用`$...$`符号<br>
> 2 当希望数学公式单独成行或者经过GFM排版之后占用多行 应当使用`$$...$$`符号<br>
> 3 对于表示条件概率 需要表示竖线的时候`|` 应当使用`\mid` 而不是直接在键盘上打出`|` => 容易被编辑器认为是一个md制表符<br>
> 4 在md引入图片的时候 不要使用`<center>`和`</center>` 在这篇文档的编辑过程中vscode的preview插件在使用了上述符号之后 导致下一段的数学公式预览显示不正常<br>
> 5 使用md的时候 单独的两段文字上下需要空出一行<br>
> 6 想要强制换行的时候 需要使用`<br>`而不是`<enter>`<br>
> 7 特殊字符如果想要避免和md解析关键字冲突 应当使用``将关键字包含在内