---
layout: post
title: "Refined Spatial Pooling"
subtitle: 'An variant of Spatial Pyramid Pooling'
author: "twistfatezz"
header-style: text
# header-img: "img/post-bg-alitrip.jpg"
# header-mask: 0.1
mathjax: true
date: 2019-11-15 16:16 
lang: ch 
catalog: true 
categories: documentation
tags:
  - ECG
  - Time 2019
---
## Abstract
目标：探究现有能够接受不定长度输出的网络结构，改进并在心电信号分类任务中应用，使得任务达到目标精度。
方法：采用MIT-BIH arrhythmia database中的心电数据进行实验，将所设计的结构和已有结构构建对照实验，根据对照结果对所设计结构的有效性进行评价。
结果：采用设计的结构，解决了已有结构中对于pooling层输入中的分辨率不能够完全利用的问题，在心电分类框架中达到了更好的效果，相比已有结构，在accuracy和recall两个指标上都有明显的提升。
研究意义：本文设计了一种可以嵌入到分类，目标识别网络框架中的基本pooling结构，对于不同尺度的输入能够产生相同的输出。相比其他具有类似功能的结构模块，在MIT-BIH arrhythmia database中分类问题框架下，得到了state-of-the-art的结果，

## Introduction
通常，神经网络中想要使用不定长度的输入的方式有：`Global average pooling(GAP)`，`ROI pooling`，`ROI align`，以及`Spatial pyramid pooling`等。其中GAP具有相对的位移不变性(tranlation invaiant)，因此可以作为不定长度分类问题模型最后一层，而对于位置敏感的目标检测问题，分割问题，则应当选择`ROI pooling`，`ROI align`，以及`Spatial pyramid pooling`等具有位置可变性的结构作为整个框架的最后层。本文构建一种针对不定长度心电信号作为神经网路后框架输入的处理层，能够对于任意长度的心电信号进行处理，能够实现对于输入的feature map利用率达到100%，并且在心电分类性能上优于上述提到的方法。常见的用于不定尺度(长度)输入的常用模型方法简介如下。


## Methodology 
关于本研究中所设计的网络通用模块的背景，原理(含有相应伪代码)，python-tensorflow代码实现在这部分展开介绍。
### Background 
「**Global average pooling**」正常使用全连接作为分类的网络模型如下图，显然使用了全连接的网络模型不支持不同尺度的输入。
<center><img src="/img/in-post/ecg_1/normal.pdf" width="80%"></center>
而gap改进的结构如下图，本质上相当于对于gap结构的输入的每个channel加入了正则。
<center><img src="/img/in-post/ecg_1/gap.pdf" width="60%"></center>

「**ROI pooling**」
[ROI pooling](/documentation/2019/10/06/post-fast-rcnn/)是fast rcnn框架中为了解决经过[seletive search](/documentation/2019/10/01/post-region-proposal/)之后产生的不同尺度roi作为全连接层输入问题而设计的结构。能够实现将不同尺度输入转化相同dimension的输出。
<center><img src="/img/in-post/fast_rcnn/roi_pooling.png" width="80%"></center>

「**ROI align**」
[ROI align](/documentation/2019/10/16/post-mask-rcnn/)是为了解决ROI pooling中对于分割等细粒度问题不够精确敏感而构建的结构。结构框架如下所示。
<center><img src="/img/in-post/mask_rcnn/roi_align.png" width="80%"></center>

「**Spatial pyramid pooling**」
如下图所示，通过将不同分辨率的输入feature map划分成相同数量的块，然后和再对于每个块中的元素做pooling(max or avg)，从而实现了对于不同尺度的特征图得到相同的输出维度。
<center><img src="/img/in-post/ecg_1/spp.pdf" width="60%"></center>


### Models 
下面介绍本文提出的改进的spatial pooling算法的细节。主要分为下面的三个步骤：<br>
1) 设置feature_map的长和宽为$a$和$b$，即feature_map= $a\times b$ <br>
2) 目的是将feature_map划分成$n\times n$个spatial_bin(pooling_filter) <br>
3) 求解每一个spatial_bin的长度和宽度 <br>

### Algorithm 
算法的伪代码如下：
<center><img src="/img/in-post/ecg_1/algorithm1.pdf" width="100%"></center>
<center><img src="/img/in-post/ecg_1/algorithm2.pdf" width="100%"></center>

### Visualization
因此，根据上述算法的伪代码，通过图像来理解算法功能，对于一个$6\times 9$的输入feature map如下，使用上述算法，对于输入的feature map进行划分，得到：
<center><img src="/img/in-post/ecg_1/fmap.pdf" width="40%"></center>

于是，使用这个算法，可以将任意的输入feature map划分成指定$n\times n$尺度的输出，并且对于输入的feature map(vector)实现100%的利用，不需要舍弃(损失)任何像素，也无需补充冗余数值(虽然对于计算机内部实现pooling并行计算的时候需要进行补值)。相比`SPPnet`中`Spatial pyramid pooling`的实现方式有明显的改进。


### Code
池化结构简述python代码如下：
```python
import math
x = []; y = [] # width & height
n = 4; a = 6; b = 9 # divide_ratio & width/height of input fmap
while n!= 0:
    xx = math.ceil(a/n)
    yy = math.ceil(b/n)
    x.append(xx)
    y.append(yy)
    a = a-xx; b = b-yy; n = n-1
print(x); print(y) # [2,2,1,1] [3,2,2,2]
```
上述构建的结构相应的tensorflow代码如下：
```python
#!/usr/bin/env python
# -*- coding:utf-8 -*-

import tensorflow as tf
import numpy as np
import math
import random


def derive_divide_list(para_a, para_m):
    """derive the split proportion list...
    :param para_a: width or height of the feature map
    :param para_m: the num of splits to the width or height
    :return: the split proportion list...
    """
    x = []  # hold the width of each spatial_bin
    while para_m != 0:
        xx = math.ceil(para_a / para_m)
        x.append(xx)
        para_a = para_a - xx
        para_m = para_m - 1
    return x


def derive_sub_feature_map_list(para_feature_map_tensor, para_d0, para_d1):
    """derive sub feature map lists...
    :param para_feature_map_tensor: feature map(a tensor) for split...
    :param para_d0: split proportion list for height of feature map
    :param para_d1: split proportion list for width of feature map
    :return: sub feature map lists...
    """
    # split firstly from dimension 1 --->height
    # s0, s1 = tf.split(value=x, num_or_size_splits=d0=[1, 2], axis=1)
    split0 = tf.split(value=para_feature_map_tensor, num_or_size_splits=para_d0, axis=1, name='split_in_height')

    # split secondly from dimension 2 --->width
    para_sub_feature_maps_dict = []
    for _ in range(len(split0)):
        # d1=[2, 1, 1]
        split1 = tf.split(value=split0[_], num_or_size_splits=para_d1, axis=2, name='split_in_width')
        # type(sub_feature_maps_dict)=list np.array(sub_feature_maps_dict).shape=(2, 3)
        para_sub_feature_maps_dict.append(list(split1))
        # xxx:
        # 对x0进行分块得到:
        # [[<tf.Tensor 'split_7:0' shape=(2, 1, 2, 2) dtype=int64>,  --> x00  x(height,width)
        #   <tf.Tensor 'split_7:1' shape=(2, 1, 1, 2) dtype=int64>,  --> x01
        #   <tf.Tensor 'split_7:2' shape=(2, 1, 1, 2) dtype=int64>], --> x02
        # 对x1进行分块得到:
        #  [<tf.Tensor 'split_8:0' shape=(2, 2, 2, 2) dtype=int64>,  --> x10
        #   <tf.Tensor 'split_8:1' shape=(2, 2, 1, 2) dtype=int64>,  --> x11
        #   <tf.Tensor 'split_8:2' shape=(2, 2, 1, 2) dtype=int64>]] --> x12
    return para_sub_feature_maps_dict


def dongh_pyramid_pooling(input_tensor, height_divide, width_divide, pooling='max', shuffle=True):
    """divide-pyramid-pooling by dongh
    :param input_tensor: with shape as (para_batch_size, height, width, channels)
    :param height_divide: integer to divide the height of input tensor
    :param width_divide: integer to divide the width of input tensor
    :param pooling: 'max'->tf.nn.max_pooling | 'avg'->tf.nn.avg_pooling
    :param shuffle: True ->shuffle the sub-divided-maps  | False ->not shuffle the sub-divided-maps
    :return: a tensor with shape:(para_batch_size, 1, height_divide*width_divide, channels)
    """
    height_for_divide = input_tensor.get_shape().as_list()[1]
    width_for_divide = input_tensor.get_shape().as_list()[2]
    input_channels = input_tensor.get_shape().as_list()[3]
    para_batch_size = tf.shape(input_tensor)[0]

    d0 = derive_divide_list(height_for_divide, height_divide)  # d0 = [2, 1]
    d1 = derive_divide_list(width_for_divide, width_divide)  # d1 = [2, 1, 1]

    # (if not shuffled: the size of sub feature maps always from bigger to smaller)
    # random split list d0/d1 to avoid unbalanced split to undermine the behavior of pooling layer...
    if shuffle:
        random.shuffle(d0)  # d0 = [1, 2]
        random.shuffle(d1)  # d1 = [2, 1, 1]
    else:
        pass

    sub_feature_maps_dict = derive_sub_feature_map_list(input_tensor, d0, d1)

    # initialize the result_tensor  dtype=tf.float32
    # result_tensor = tf.zeros([batch_size, 1, 1, input_channels])
    result_tensor = tf.zeros([para_batch_size, 1, 1, input_channels])
    for i in range(np.array(sub_feature_maps_dict).shape[0]):  # i=height
        for j in range(np.array(sub_feature_maps_dict).shape[1]):  # j=width
            pool_height = sub_feature_maps_dict[i][j].get_shape().as_list()[1]
            pool_width = sub_feature_maps_dict[i][j].get_shape().as_list()[2]
            # sub_feature_map_pool_output.shape=(para_batch_size, 1, 1, channels)
            if pooling == 'max':
                sub_feature_map_pool_output = tf.nn.max_pool(sub_feature_maps_dict[i][j],
                                                             ksize=[1, pool_height, pool_width, 1],
                                                             strides=[1, pool_height, pool_width, 1],
                                                             padding='VALID', data_format="NHWC", name=None)
            elif pooling == 'avg':
                sub_feature_map_pool_output = tf.nn.avg_pool(sub_feature_maps_dict[i][j],
                                                             ksize=[1, pool_height, pool_width, 1],
                                                             strides=[1, pool_height, pool_width, 1],
                                                             padding='VALID', data_format="NHWC", name=None)
            else:
                sub_feature_map_pool_output = None
                exit("wrong type of pooling!")
            # use tf.concat() to get the sub_pool output together...
            result_tensor = tf.concat([result_tensor, sub_feature_map_pool_output], axis=2, name="concat")
    #
    pyramid_sub_feature_output = tf.slice(input_=result_tensor, begin=[0, 0, 1, 0], size=[-1, 1, height_divide * width_divide, input_channels])
    return pyramid_sub_feature_output

```

## Application
课题研究设计使用`MIT-BIH arrhythmia database`进行相应的心电信号分析，分类。在课题研究中，上述数据库中的心电信号按照心电节拍进行标注，然而对于节拍分类任务而言，每个节拍所包含的心电信号采样点数差别很大，较短的波形仅仅包含120个采样点，而有的波形可包含大约520个采样点(数据库采样频率为360Hz)，为了能够解决对于不定长度输出的问题，我探究了本文中在Method部分Background小节中提及的种种方法进行尝试，效果不能够达到课题目标中的要求。特别的，对于spatial pooling的方式，我发现，在网络上的实现中，存在漏洞：当spatial pooling层的输入的feature map的分辨率不能够被所期望划分的块数整除的时候，采用舍弃feature map中部分分辨率的方法，进行处理。这是导致上述方法不能够在心电处理分析任务中达到理想结果的一个较为重要的原因。对于给定的心电信号如下图所示。
<center><img src="/img/in-post/ecg_1/ecgwave.pdf" width="30%"></center>

如果按照传统的spatial pyramid pooling的方法，由于希望被划分的块数不能够被输入feature map的尺度所整除，可能需要舍弃掉输入图像经过模型inference产生的用于pooling层的输入feature map的部分边缘分辨像素，对于一个分类模型而言，一般来说，pooling层用于对于不同尺度输入产生相同输出，因此，该结构常位于整个模型的最后，根据输入特征图中关于原图的[感受野](/documentation/2019/10/03/post-compute-receptive-field/)之间的关系，易知，网络模型的最后层的一个cell映射回原图的感受野非常大，删除或者添加一个元素将可能对整个网络模型产生很大的影像，因此，传统spatial pyramid pooling的方法中对于特征图的划分需要优化。采用上述伪代码表示的算法，可以对于输出的feature map实现100%的利用，而且无须删除、补充元素。

## Results
针对MIT-BIH每个record进行统计AAMI各个类别的数量如下图：
<center><img src="/img/in-post/ecg_1/statistics.pdf" width="100%"></center>
针对心电信号分类使用中采用的网络模型backbone参见下图：
<center><img src="/img/in-post/ecg_1/backbone.pdf" width="100%"></center>
关于backbone的对照实验在本文不做赘述，此处直接选择ISEnet-14模型作为关于本文设计的结构的对照实验的基础。其中分类所使用的pooling模块对照结构有`Global average pooling`，`ROI pooling`，`ROI align`，`Spatial pyramid pooling`，`Refined spatial pooling`。

对于AAMI规定的5分类问题而言，采用相同的训练测试数据。所有的指标的获取都采用"1 vs rest"策略得到。为了能够均衡，无偏地比较各种pooling结构性能的好坏，马修斯相关性系数被用于作为唯一多分类评价指标。其各项指标的计算公式列举如下。

$$
Acc=\frac{TP+TN}{TP+TN+FP+FN}
$$

$$
Sen=\frac{TP}{TP+FN}
$$

$$
Spe=\frac{TN}{TN+FP}
$$

$$
Ppr=\frac{TP}{TP+FP}
$$

$$
MCC=\frac{TP/N-S\times P}{\sqrt{PS(1-S)(1-P)}}
$$

其中各个符号的含义：TP(true positive),TN(true negative)，FP(false positive)，FN(false negative)，MCC(Matthews correlation coefficient)，$N=TN+TP+FN+FP$，$S=(TP+FN)/N$，$P=(TP+FP)/N$。

得到的各个pooling模块的结果对比如下表所示。
<center><img src="/img/in-post/ecg_1/mcc.pdf" width="60%"></center>
使用所设计的结构，进行AAMI5分类问题得到的结果可视化为confusion matrix如下图所示。
<center><img src="/img/in-post/ecg_1/confusion_matrix.pdf" width="40%"></center>

## Reference

> 1 当使用inline数学公式且公式经过GFM排版之后都在同一行 使用`$...$`符号<br>
> 2 当希望数学公式单独成行或者经过GFM排版之后占用多行 应当使用`$$...$$`符号<br>
> 3 对于表示条件概率 需要表示竖线的时候`|` 应当使用`\mid` 而不是直接在键盘上打出`|` => 容易被编辑器认为是一个md制表符<br>
> 4 在md引入图片的时候 不要使用`<center>`和`</center>` 在这篇文档的编辑过程中vscode的preview插件在使用了上述符号之后 导致下一段的数学公式预览显示不正常<br>
> 5 使用md的时候 单独的两段文字上下需要空出一行<br>
> 6 想要强制换行的时候 需要使用`<br>`而不是`<enter>`<br>
> 7 特殊字符如果想要避免和md解析关键字冲突 应当使用``将关键字包含在内