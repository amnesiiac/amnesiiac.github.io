---
layout: post
title: "YOLO v3"
subtitle: '原理解析|结合原文分析'
author: "twistfatezz"
header-style: text
# header-img: "img/post-bg-alitrip.jpg"
# header-mask: 0.1
mathjax: true
date: 2019-10-12 9:46 
lang: ch 
catalog: true 
categories: documentation
tags:
  - object detection
  - Time 2019
---

## Introduction
yolo v3在[yolo v2](/documentation/2019/10/09/post-yolo2/)的基础进行了一些改进，这些更改使其效果变得更好。在320×320的图像上，yolo v3运行速度达到了22.2毫秒，mAP为28.2。其与SSD一样准确，但速度快了三倍。本文将按照yolo v3的介绍顺序进行分步解析，对于yolo v3中借鉴了yolo v2的部分，不展开介绍。



## Net Structure 
根据官方源码绘制yolo v3的整体结构图如下，方便对于整体框架进行理解。里面没有使用很复杂的网络技巧，但是巧妙的生成了3种不同尺度的feature map用于多尺度预测。

<center><img src="/img/in-post/yolo_v3/structure.png" width="100%"></center>

上图中关于darknet-53的结构详细展示，下面的图片来自原论文。应当注意的是，不同于[yolo v2](/documentation/2019/10/09/post-yolo2/)，v3的框架中没有`pooling`层，即v3的网络通过设置conv的stride=2(共有5个stride=2的conv层)实现feature map大小的改变，对应于v2中的5层pooling用于改变feature map的大小。

<center><img src="/img/in-post/yolo_v3/darknet_53.png" width="40%"></center>

## Details
「**Predictions Across Scales**」yolo v3总共通过3中不同尺度大小的特征图对于物体进行检测，分别是13x13，26x26，52x52。这些不同的尺度的划分使得在输入图像上的小格子大小不同，对应到不同尺度的feature map上去分别做bbox以及confidence的预测，结合上面绘制的结构图很容易理解。

「**anchor boxes**」yolov3 anchor box一共有9个，比v2多了4个，同样由k-means聚类得到。在COCO数据集上，9个聚类中心对应的bbox尺寸是：(10×13),(16×30),(33×23),(30×61),(62×45),(59×119),(116×90),(156×198),(373×326)。不同尺寸特征图对应不同大小的先验框。对应关系整理如下：<br>
----- 13×13 feature map上预测的anchor box尺度 [(116×90), (156×198), (373×326)] <br>
----- 26x26 feature map上预测的anchor box尺度 [(30×61), (62×45), (59×119)] <br>
----- 52×52 feature map上预测的anchor box尺度 [(10×13), (16×30), (33×23)] <br>
input feature map上划分的小格子尺寸越小，其prior bbox的尺寸越小，这其实很符合常理，没什么可解释的。

「**使用多个2分类交叉熵函数代替一个softmax交叉熵函数的原因**」直接参考原文即可：
> We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. Dur- ing training we use binary cross-entropy loss for the class predictions.
This formulation helps when we move to more complex domains like the Open Images Dataset [7]. In this dataset there are many overlapping labels (i.e. Woman and Person).

## Things Didn’t Work
「**Anchor box x, y offset predictions**」根据原论文进行理解：
> We tried using the normal anchor box prediction mechanism where you predict the x, y offset as a multiple of the box width or height using a linear activation. We found this formulation decreased model stability and didn’t work very well.

「**Linear x, y predictions instead of logistic**」根据原论文进行理解：
> We tried using a linear activation to directly predict the x, y offset instead of the logistic activation. This led to a couple point drop in mAP.

「**Focal loss**」根据原论文进行理解：
> We tried using focal loss. It dropped our mAP about 2 points. YOLOv3 may already be robust to the problem focal loss is trying to solve because it has separate objectness predictions and conditional class predictions. Thus for most examples there is no loss from the class predictions? Or something? We aren’t totally sure.

更多关于focal loss的内容，参考

「**Dual IOU thresholds and truth assignment**」作者说采用了一种类似faster-rcnn指定的两个阈值用于区分正负样本的策略进行实验，得到的结构不好，并且表示`喜欢`yolo v2/v3的定义正负样本的策略，不打算改动。关于正负样本的策略，见[yolo v2](/documentation/2019/10/09/post-yolo2/)中的loss function部分即可。
> Faster RCNN uses two IOU thresholds during training. If a prediction overlaps the ground truth by .7 it is as a positive exam- ple, by [.3 − .7] it is ignored, less than .3 for all ground truth objects it is a negative example. We tried a similar strategy but couldn’t get good results.
We quite like our current formulation, it seems to be at a local optima at least. It is possible that some of these techniques could eventually produce good results, perhaps they just need some tuning to stabilize the training.

## Loss Function
loss function的5个部分和[yolo v2](/documentation/2019/10/09/post-yolo2/)一致，不做介绍。

$$
\text {loss}(\text {object})=\lambda_{\text {coord}} \sum_{i=0}^{S \times S} \sum_{j=0}^{B} I_{i j}^{\text {obj}}\left[\left(x_{i}-\hat{x}_{i}\right)^{2}+\left(y_{i}-\hat{y}_{i}\right)^{2}\right]+
$$

$$
\lambda_{\text {coord}} \sum_{i=0}^{S\times S} \sum_{j=0}^{B} I_{ij}^{\text {obj}}\left(2-w_{i} \times h_{i}\right)\left[\left(w_{i}-\hat{w}_{i}\right)^{2}+\left(h_{i}-\hat{h}_{i}\right)^{2}\right]-
$$

$$
\sum_{i=0}^{S \times S} \sum_{j=0}^{B} I_{i j}^{\text {obj}}\left[\hat{C}_{i} \log \left(C_{i}\right)+\left(1-\hat{C}_{i}\right) \log \left(1-C_{i}\right)\right]-
$$

$$
\lambda_{\text {noobj}} \sum_{i=0}^{S \times S} \sum_{j=0}^{B} I_{i j}^{\text {noobj}}\left[\hat{C}_{i} \log \left(C_{i}\right)+\left(1-\hat{C}_{i}\right) \log \left(1-C_{i}\right)\right]-
$$

$$
\sum_{i=0}^{S\times S} I_{i j}^{\text {obj}} \sum_{c\in class}\left[\hat{p}_{i}(c) \log \left(p_{i}(c)\right)+\left(1-\hat{p}_{i}(c)\right) \log \left(1-p_{i}(c)\right)\right]
$$

## Code
> https://github.com/twistfatezz/tensorflow-yolov3
等待整理...

## Reference
> 部分翻译自原文 <br>
> https://blog.csdn.net/leviopku/article/details/82660381

> 1 当使用inline数学公式且公式经过GFM排版之后都在同一行 使用`$...$`符号<br>
> 2 当希望数学公式单独成行或者经过GFM排版之后占用多行 应当使用`$$...$$`符号<br>
> 3 对于表示条件概率 需要表示竖线的时候`|` 应当使用`\mid` 而不是直接在键盘上打出`|` => 容易被编辑器认为是一个md制表符<br>
> 4 在md引入图片的时候 不要使用`<center>`和`</center>` 在这篇文档的编辑过程中vscode的preview插件在使用了上述符号之后 导致下一段的数学公式预览显示不正常<br>
> 5 使用md的时候 单独的两段文字上下需要空出一行<br>
> 6 想要强制换行的时候 需要使用`<br>`而不是`<enter>`<br>
> 7 特殊字符如果想要避免和md解析关键字冲突 应当使用``将关键字包含在内