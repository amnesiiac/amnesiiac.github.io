---
layout: post
title: "monte carlo method"
author: "melon"
date: 2022-10-30 22:14
categories: "2022" 
tags:
  - math
---

monte carlo was originally the name of a casino, probably because
the monte carlo method is a random simulation method,
much like the process of throwing dice in a casino.

monte carlo methods are a broad class of computational algorithms that rely on
repeated random sampling to obtain numerical results.
the underlying concept is to use randomness to solve problems that
might be deterministic in principle.  

monte carlo methods are mainly used in three problem classes: optimization,
numerical integration, and generating samples from a probability distribution.

in principle, monte carlo methods can be used to solve any problem
with probabilistic interpretation.
according to large-numbers-law, the expected value of some random variable
in the form of integrals can be approximated by taking the empirical mean of
independent samples of the variable.

<hr>

### # mento carlo process
(1) pick a space of possible samples.  
(2) randomly sample in the space above by its probability distribution.  
(3) perform defined operations on the random samples.  
(4) aggregate the data.

<hr>

### # estimating pi: a preliminary exploration 
(1) possible sample space should be the circle inscribed within a square.  
(2) the space is then randomly sampled by throwing needles into it.  
(3) a defined operation is performed: counting the needles.  
(4) and finally, a ratio is calculated for those needles.

given the formula as follows, we can estimate the ratio between the circle and
the square by throwing needles randomly.

$$ \frac{Area\_circle}{Area\_square} = \frac{\pi r^2}{r^2} = \pi $$

throwing the needles, with needles number growing, the estimation result is approaching $$\pi$$.

<img src="https://cdn.jsdelivr.net/gh/slothfull/cdn@main/image/mc.pdf" width="600"/>

python code for the above experiment.
```text
from random import random
from math import pow, sqrt

trial=1000000; hits = 0; throws = 0
for i in range (1, trial):
    throws += 1
    x = random()                        # [0-1]
    y = random()                        # [0-1]
    dist = sqrt(pow(x, 2) + pow(y, 2))
    if dist <= 1.0:
        hits = hits + 1.0

pi = 4 * (hits / throws)                # hits/throws = 1/4 Pi
print(f'pi={pi}')
```

<hr>

### # estimating function integrals: from example to principle 
sometimes, it's hard to get the math formula of f(x),
and make it impossible to compute integral,
thus we could use monte carlo Method to mimic the approximate value.

$$ \theta=\int_a^b f(x) dx $$


(1) attempt 1:
to roughly estimate the integral above, we could just choose one $$x_0$$ between $$a$$ and $$b$$,
the result can be estimated as follows:

$$ \theta=(b-a)f(x_0)$$

<img src="https://cdn.jsdelivr.net/gh/slothfull/cdn@main/image/mc2.pdf" width="400"/>

(2) attempt 2:
the 1st attempt seldom meet the need prescision, try more sampling points, and use the average
value of them to approach to the truth avg value of all the points between $$a$$ and $$b$$:

$$ \theta=(b-a)\times \frac{1}{n}\sum_{i=0}^{n-1}f(x_i)$$

(3) attempt 3:
the 2nd attempt may get closer, but the average operation above implies that
the independent variables follow a uniform distribution.
generalize the uniform distribution to arbitary distribution,
we can solve the integral as follows:

$$ \theta=\int_a^b f(x)dx = \int_a^b \frac{f(x)}{g(x)} g(x)dx = E[\frac{f(x)}{g(x)}] $$

in which $$g(x)$$ denotes the distribution of $$x$$.  
according to the law-of-large-numbers:
the above expectation can be approximated by sampling, when the sample size $$n\to \infty$$,
the average of sampling observation approches to the mathemetical expectation, which gives us:

$$ \frac{1}{n}\sum_{i=0}^{n}h(x_i) \to \mathbb{E}[h(x)]$$

thus, let $$h(x)=\frac{f(x)}{g(x)}$$, the solution of integral above is as follows:

$$\frac{1}{n}\sum_{i=0}^{n-1}\frac{f(x_i)}{g(x_i)} \to E[\frac{f(x)}{g(x)}] = \int_a^b \frac{f(x)}{g(x)}g(x)dx = \theta $$

since $$f(x)$$ is known function, now the solution narrow down to
hwo to perform sampling from the distribution of $$x$$.

<hr>

### # sampling from abnormal distribution: acceptance-rejection-sampling
for normal distribution like uniform 0-1 distribution, the samples (pseudo random numbers)
can be generated by linear-congruential-generator easily.

moreover, other distributions like: 2D normal distribution, t-distribution, f-distribution,
beta-distribution, gamma-distribution can be derived from the transformation of
the samples derived from uniform 0-1.

however, not all distribution can be dervied as above, we need other methods to get over it.
a feasible approach is to use acceptance-rejection-sampling, which is used to obtain
samples of certain distribution.

if $$g(x)$$ is too complicated for sampling directly,
we can first examine a feasible distribution $$q(x)$$(e.g. gaussian),
then sampling in gaussian distribution and reject the samples according to some criteria
to approach $$g(x)$$.

<img src="https://cdn.jsdelivr.net/gh/slothfull/cdn@main/image/mc3.pdf" width="400"/>

(1) given $$q(x)$$ as a "easy-for-sampling" distribution,
a constant $$k$$ to enable $$kq(x)>g(x)$$ for all x.  
(2) sampling from $$q(x)$$ to get a sample $$z_0$$ by linear-congruential-generator.  
(3) sampling from uniform distribution $$(0,kq(z_0))$$ to get a sample $$u0$$.  
(4) if $$u0$$ appear in the grey part of the above graph, reject the sample, else accept it.  
(5) repeat the step 4 to get n accepted sample: $$z_0, z_1...,z_{n-1}$$.  
(6) finally, the monte carlo method result can be estimated with $$\frac{1}{n}\sum_{i=0}^{n-1}\frac{q(z_i)}{g(z_i)}$$

drawbacks of acceptance-rejection-sampling:  
(1) for 2d distribution $$p(x,y)$$, sometimes it's only practical to get its conditional
format: $$p(x\mid y)$$ or $$p(y\mid x)$$. so it's impossible to adopt acceptance-rejection
sampling to get samples for $$p(x,y)$$.  
(2) for some complicated & abnormal distributions $$p(x1,x2...xn)$$, hardly can we find
suitable q(x) and k for perform acceptance-rejection-sampling.

<hr>

### # variants of monte carlo method
(1) monte carlo tree search (MCTS) used in go game:  
MCTS is a heuristic search algorithm for some kinds of decision processes,
most notably those employed in software that plays board games.
in that context MCTS is used to solve the game tree.

(2) markov chain monte carlo method (MCMC):  
when the probability distribution of the variable is parameterized,
mathematicians often use a MCMC sampler,
which idea is to design a markov chain model with a prescribed stationary probability distribution.
subject to the limit, the samples generated by MCMC will conform to the desired distribution.
by the ergodic theorem, the stationary distribution is approximated by
the empirical measures of the random states of the MCMC sampler.

<hr>

### # reference
https://en.wikipedia.org/wiki/Monte_Carlo_method  
https://en.wikipedia.org/wiki/Monte_Carlo_tree_search  
https://www.cnblogs.com/pinard/p/6625739.html
